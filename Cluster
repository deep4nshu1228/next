# Complete Warranty Forecasting with Bike Model Clustering Pipeline
# Import all required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, mean_absolute_error, mean_squared_error
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import warnings
warnings.filterwarnings('ignore')

# Set display options
pd.set_option('display.max_columns', None)
plt.style.use('default')
plt.rcParams['figure.figsize'] = (12, 8)

print("‚úÖ All libraries imported successfully!")

# =============================================================================
# 1. DATA PREPARATION AND SAMPLE DATA CREATION
# =============================================================================

# Create sample warranty data (replace with your actual data loading)
# df = pd.read_csv('your_warranty_data.csv')

np.random.seed(42)
bike_models = [f'Model_{i:02d}' for i in range(1, 25)]
months = pd.date_range('2022-01', '2024-12', freq='M')

sample_data = []
for model in bike_models:
    base_cost = np.random.uniform(50, 200)
    seasonality = np.random.uniform(0.8, 1.2, 12)
    
    for month in months:
        dispatch_month = month - pd.DateOffset(months=np.random.randint(1, 13))
        seasonal_factor = seasonality[month.month - 1]
        volume = np.random.randint(100, 2000)
        age = (month - dispatch_month).days // 30
        
        warranty_cost = (base_cost * volume * seasonal_factor * 
                        (1 + age * 0.02) * np.random.uniform(0.7, 1.3))
        
        sample_data.append({
            'BIKE_GROUPED': model,
            'DISPATCH_MONTH': dispatch_month.strftime('%m-%Y'),
            'WARRANTY_MONTH': month.strftime('%m-%Y'),
            'AGE': age,
            'VOLUME': volume,
            'WARRANTY_COST': warranty_cost,
        })

df = pd.DataFrame(sample_data)
df['COST_PER_UNIT'] = df['WARRANTY_COST'] / df['VOLUME']

print(f"‚úÖ Data loaded! Shape: {df.shape}, Models: {df['BIKE_GROUPED'].nunique()}")

# =============================================================================
# 2. TIME SERIES CREATION AND FEATURE ENGINEERING
# =============================================================================

# Convert date columns
df['DISPATCH_DATE'] = pd.to_datetime(df['DISPATCH_MONTH'], format='%m-%Y')
df['WARRANTY_DATE'] = pd.to_datetime(df['WARRANTY_MONTH'], format='%m-%Y')
df['WARRANTY_LAG'] = (df['WARRANTY_DATE'] - df['DISPATCH_DATE']).dt.days / 30.44

# Create time series format
warranty_ts = df.pivot_table(
    index='WARRANTY_DATE',
    columns='BIKE_GROUPED',
    values='WARRANTY_COST',
    aggfunc='sum',
    fill_value=0
)

cost_per_unit_ts = df.pivot_table(
    index='WARRANTY_DATE',
    columns='BIKE_GROUPED', 
    values='COST_PER_UNIT',
    aggfunc='mean',
    fill_value=0
)

print(f"‚úÖ Time series created: {warranty_ts.shape}")

# =============================================================================
# 3. STATISTICAL FEATURE EXTRACTION (Simplified TSFresh alternative)
# =============================================================================

def extract_statistical_features(ts_data):
    """Extract statistical features from time series"""
    features = pd.DataFrame(index=ts_data.columns)
    
    for col in ts_data.columns:
        series = ts_data[col]
        
        # Basic statistics
        features.loc[col, 'mean'] = series.mean()
        features.loc[col, 'std'] = series.std()
        features.loc[col, 'min'] = series.min()
        features.loc[col, 'max'] = series.max()
        features.loc[col, 'median'] = series.median()
        features.loc[col, 'skewness'] = series.skew()
        features.loc[col, 'kurtosis'] = series.kurtosis()
        
        # Trend features
        if len(series) > 2:
            features.loc[col, 'trend'] = np.polyfit(range(len(series)), series.values, 1)[0]
        else:
            features.loc[col, 'trend'] = 0
            
        # Seasonal features
        monthly_avg = series.groupby(series.index.month).mean()
        features.loc[col, 'seasonal_variance'] = monthly_avg.std()
        features.loc[col, 'peak_month'] = monthly_avg.idxmax()
        features.loc[col, 'peak_to_avg_ratio'] = monthly_avg.max() / monthly_avg.mean() if monthly_avg.mean() > 0 else 0
        
        # Rolling statistics
        features.loc[col, 'rolling_mean_3'] = series.rolling(3).mean().mean()
        features.loc[col, 'rolling_std_3'] = series.rolling(3).std().mean()
        
        # Autocorrelation
        if len(series) > 1:
            features.loc[col, 'autocorr_lag1'] = series.autocorr(lag=1)
        else:
            features.loc[col, 'autocorr_lag1'] = 0
            
    return features.fillna(0)

# Extract features
print("üîÑ Extracting statistical features...")
warranty_features = extract_statistical_features(warranty_ts)
cost_features = extract_statistical_features(cost_per_unit_ts)

# Add business-specific features
business_features = pd.DataFrame(index=warranty_ts.columns)
for model in warranty_ts.columns:
    model_data = df[df['BIKE_GROUPED'] == model]
    business_features.loc[model, 'avg_volume'] = model_data['VOLUME'].mean()
    business_features.loc[model, 'avg_age'] = model_data['AGE'].mean()
    business_features.loc[model, 'avg_warranty_lag'] = model_data['WARRANTY_LAG'].mean()
    business_features.loc[model, 'total_warranty_cost'] = model_data['WARRANTY_COST'].sum()

# Combine all features
all_features = pd.concat([
    warranty_features.add_prefix('warranty_'),
    cost_features.add_prefix('cost_'),
    business_features
], axis=1).fillna(0)

print(f"‚úÖ Features extracted: {all_features.shape[1]} features for {all_features.shape[0]} models")

# =============================================================================
# 4. FEATURE SCALING AND DIMENSIONALITY REDUCTION
# =============================================================================

scaler = StandardScaler()
features_scaled = scaler.fit_transform(all_features)

pca = PCA(n_components=0.85)
features_pca = pca.fit_transform(features_scaled)

print(f"‚úÖ PCA completed: {features_pca.shape[1]} components, {pca.explained_variance_ratio_.sum():.3f} variance explained")

# =============================================================================
# 5. OPTIMAL CLUSTER FINDING
# =============================================================================

print("üîç Finding optimal number of clusters...")
max_clusters = min(12, len(features_pca)//3)
k_range = range(2, max_clusters)
inertias = []
silhouette_scores = []

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)
    labels = kmeans.fit_predict(features_pca)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(features_pca, labels))

# Find optimal k
best_sil_k = k_range[np.argmax(silhouette_scores)]
optimal_k = best_sil_k
print(f"‚úÖ Optimal clusters: {optimal_k} (Silhouette score: {max(silhouette_scores):.3f})")

# =============================================================================
# 6. PERFORM CLUSTERING
# =============================================================================

print("üå≥ Performing clustering...")

# Hierarchical clustering
hierarchical_clustering = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')
hier_labels = hierarchical_clustering.fit_predict(features_pca)

# K-means clustering
kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=20)
kmeans_labels = kmeans_final.fit_predict(features_pca)

# Create cluster mapping
cluster_df = pd.DataFrame({
    'bike_model': all_features.index,
    'hierarchical_cluster': hier_labels,
    'kmeans_cluster': kmeans_labels
})

# Use hierarchical clustering as primary method
cluster_df['cluster'] = cluster_df['hierarchical_cluster']

print(f"‚úÖ Clustering completed!")
print("Cluster distribution:")
print(cluster_df['cluster'].value_counts().sort_index())

# =============================================================================
# 7. CLUSTER ANALYSIS AND VISUALIZATION
# =============================================================================

print("üìä Analyzing clusters...")

# Analyze cluster characteristics
cluster_stats = []
for cluster_id in sorted(cluster_df['cluster'].unique()):
    cluster_models = cluster_df[cluster_df['cluster'] == cluster_id]['bike_model'].tolist()
    cluster_warranty_cost = warranty_ts[cluster_models].sum(axis=1)
    
    stats = {
        'cluster': cluster_id,
        'n_models': len(cluster_models),
        'avg_monthly_cost': cluster_warranty_cost.mean(),
        'cost_volatility': cluster_warranty_cost.std(),
        'total_cost': cluster_warranty_cost.sum(),
        'models': cluster_models[:3]  # Show first 3 models
    }
    cluster_stats.append(stats)

cluster_summary = pd.DataFrame(cluster_stats)
print("\nüìã Cluster Summary:")
print(cluster_summary[['cluster', 'n_models', 'avg_monthly_cost', 'cost_volatility']])

# Visualize clusters
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Plot 1: Elbow curve
axes[0,0].plot(k_range, inertias, 'bo-')
axes[0,0].set_xlabel('Number of Clusters')
axes[0,0].set_ylabel('Inertia')
axes[0,0].set_title('Elbow Method')
axes[0,0].grid(True, alpha=0.3)

# Plot 2: Silhouette scores
axes[0,1].plot(k_range, silhouette_scores, 'ro-')
axes[0,1].axvline(x=optimal_k, color='g', linestyle='--', label=f'Optimal k={optimal_k}')
axes[0,1].set_xlabel('Number of Clusters')
axes[0,1].set_ylabel('Silhouette Score')
axes[0,1].set_title('Silhouette Analysis')
axes[0,1].legend()
axes[0,1].grid(True, alpha=0.3)

# Plot 3: PCA visualization
scatter = axes[1,0].scatter(features_pca[:, 0], features_pca[:, 1], 
                           c=cluster_df['cluster'], cmap='viridis', alpha=0.7)
axes[1,0].set_xlabel('First Principal Component')
axes[1,0].set_ylabel('Second Principal Component')
axes[1,0].set_title('Clusters in PCA Space')
plt.colorbar(scatter, ax=axes[1,0])

# Plot 4: Cluster sizes
cluster_counts = cluster_df['cluster'].value_counts().sort_index()
axes[1,1].bar(cluster_counts.index, cluster_counts.values, color='skyblue')
axes[1,1].set_xlabel('Cluster ID')
axes[1,1].set_ylabel('Number of Models')
axes[1,1].set_title('Models per Cluster')
axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# =============================================================================
# 8. PREPARE MODELING DATA
# =============================================================================

print("üîß Preparing modeling features...")

# Merge cluster information with original data
df_clustered = df.merge(cluster_df[['bike_model', 'cluster']], 
                       left_on='BIKE_GROUPED', right_on='bike_model', how='left')

# Create comprehensive modeling features
def create_modeling_features(df_input):
    df_model = df_input.copy()
    
    # Date features
    df_model['dispatch_month'] = df_model['DISPATCH_DATE'].dt.month
    df_model['dispatch_quarter'] = df_model['DISPATCH_DATE'].dt.quarter
    df_model['warranty_month'] = df_model['WARRANTY_DATE'].dt.month
    df_model['warranty_quarter'] = df_model['WARRANTY_DATE'].dt.quarter
    df_model['dispatch_year'] = df_model['DISPATCH_DATE'].dt.year
    df_model['warranty_year'] = df_model['WARRANTY_DATE'].dt.year
    
    # Cyclical encoding for months
    df_model['dispatch_month_sin'] = np.sin(2 * np.pi * df_model['dispatch_month'] / 12)
    df_model['dispatch_month_cos'] = np.cos(2 * np.pi * df_model['dispatch_month'] / 12)
    df_model['warranty_month_sin'] = np.sin(2 * np.pi * df_model['warranty_month'] / 12)
    df_model['warranty_month_cos'] = np.cos(2 * np.pi * df_model['warranty_month'] / 12)
    
    # Volume-based features
    df_model['log_volume'] = np.log1p(df_model['VOLUME'])
    df_model['volume_squared'] = df_model['VOLUME'] ** 2
    df_model['volume_sqrt'] = np.sqrt(df_model['VOLUME'])
    
    # Age-related features
    df_model['age_squared'] = df_model['AGE'] ** 2
    df_model['age_log'] = np.log1p(df_model['AGE'])
    df_model['age_sqrt'] = np.sqrt(df_model['AGE'])
    
    # Interaction features
    df_model['age_volume_interaction'] = df_model['AGE'] * df_model['VOLUME']
    df_model['age_month_interaction'] = df_model['AGE'] * df_model['warranty_month']
    df_model['volume_month_interaction'] = df_model['VOLUME'] * df_model['warranty_month']
    
    # Lag features (sorted by bike model and date)
    df_model = df_model.sort_values(['BIKE_GROUPED', 'WARRANTY_DATE'])
    df_model['warranty_cost_lag1'] = df_model.groupby('BIKE_GROUPED')['WARRANTY_COST'].shift(1)
    df_model['warranty_cost_lag2'] = df_model.groupby('BIKE_GROUPED')['WARRANTY_COST'].shift(2)
    df_model['warranty_cost_ma3'] = df_model.groupby('BIKE_GROUPED')['WARRANTY_COST'].rolling(3, min_periods=1).mean().values
    df_model['cost_per_unit_ma3'] = df_model.groupby('BIKE_GROUPED')['COST_PER_UNIT'].rolling(3, min_periods=1).mean().values
    
    return df_model.fillna(0)

df_model = create_modeling_features(df_clustered)

# Feature columns for modeling
feature_cols = [col for col in df_model.columns 
               if col not in ['WARRANTY_COST', 'BIKE_GROUPED', 'bike_model',
                            'DISPATCH_MONTH', 'WARRANTY_MONTH', 'DISPATCH_DATE', 'WARRANTY_DATE']]

print(f"‚úÖ Modeling features created: {len(feature_cols)} features")

# =============================================================================
# 9. TRAIN CLUSTER-SPECIFIC MODELS
# =============================================================================

print("üöÄ Training cluster-specific models...")

cluster_models = {}
model_performance = []

for cluster_id in sorted(df_model['cluster'].unique()):
    print(f"\nüìà Training model for Cluster {cluster_id}")
    
    # Filter data for current cluster
    cluster_data = df_model[df_model['cluster'] == cluster_id].copy()
    
    if len(cluster_data) < 30:
        print(f"‚ö†Ô∏è  Cluster {cluster_id} has only {len(cluster_data)} samples - using Random Forest")
        model_type = 'rf'
    else:
        model_type = 'lgb'
    
    # Prepare features and target
    X = cluster_data[feature_cols]
    y = cluster_data['WARRANTY_COST']
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, shuffle=True
    )
    
    # Train model based on cluster size
    if model_type == 'lgb':
        model = lgb.LGBMRegressor(
            n_estimators=500,
            learning_rate=0.1,
            max_depth=6,
            num_leaves=31,
            min_child_samples=20,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1,
            verbosity=-1
        )
        
        model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
        )
    else:
        model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        model.fit(X_train, y_train)
    
    # Evaluate model
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    
    cluster_models[cluster_id] = {
        'model': model,
        'features': feature_cols,
        'mae': mae,
        'rmse': rmse,
        'mape': mape,
        'n_samples': len(cluster_data),
        'model_type': model_type
    }
    
    model_performance.append({
        'cluster': cluster_id,
        'n_samples': len(cluster_data),
        'mae': mae,
        'rmse': rmse,
        'mape': mape,
        'model_type': model_type
    })
    
    print(f"‚úÖ Cluster {cluster_id}: MAE={mae:.0f}, RMSE={rmse:.0f}, MAPE={mape:.1f}%, Type={model_type}")

# =============================================================================
# 10. MODEL PERFORMANCE ANALYSIS
# =============================================================================

performance_df = pd.DataFrame(model_performance)
print(f"\nüìä Overall Model Performance:")
print(performance_df)

# Calculate baseline model (single model for all data)
print(f"\nüîÑ Training baseline model (no clustering)...")
X_baseline = df_model[feature_cols]
y_baseline = df_model['WARRANTY_COST']
X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(
    X_baseline, y_baseline, test_size=0.25, random_state=42
)

baseline_model = lgb.LGBMRegressor(
    n_estimators=500,
    learning_rate=0.1,
    max_depth=6,
    random_state=42,
    verbosity=-1
)
baseline_model.fit(X_train_base, y_train_base)
y_pred_baseline = baseline_model.predict(X_test_base)

baseline_mae = mean_absolute_error(y_test_base, y_pred_baseline)
baseline_rmse = np.sqrt(mean_squared_error(y_test_base, y_pred_baseline))
baseline_mape = np.mean(np.abs((y_test_base - y_pred_baseline) / y_test_base)) * 100

print(f"üìà Baseline Model: MAE={baseline_mae:.0f}, RMSE={baseline_rmse:.0f}, MAPE={baseline_mape:.1f}%")

# Compare performance
clustered_weighted_mae = np.average(performance_df['mae'], weights=performance_df['n_samples'])
improvement = ((baseline_mae - clustered_weighted_mae) / baseline_mae) * 100

print(f"üéØ Clustered Model Weighted MAE: {clustered_weighted_mae:.0f}")
print(f"üöÄ Improvement over baseline: {improvement:.1f}%")

# =============================================================================
# 11. PREDICTION FUNCTION
# =============================================================================

def predict_warranty_cost(new_data, cluster_models, cluster_df, scaler, feature_cols):
    """
    Make predictions using cluster-specific models
    """
    # Add cluster information
    new_data_clustered = new_data.merge(
        cluster_df[['bike_model', 'cluster']], 
        left_on='BIKE_GROUPED', 
        right_on='bike_model', 
        how='left'
    )
    
    # Create features
    new_data_features = create_modeling_features(new_data_clustered)
    
    predictions = []
    
    for cluster_id in sorted(cluster_models.keys()):
        cluster_data = new_data_features[new_data_features['cluster'] == cluster_id].copy()
        
        if len(cluster_data) == 0:
            continue
            
        model_info = cluster_models[cluster_id]
        model = model_info['model']
        
        X = cluster_data[feature_cols].fillna(0)
        y_pred = model.predict(X)
        
        cluster_predictions = cluster_data[['BIKE_GROUPED', 'WARRANTY_MONTH', 'VOLUME', 'AGE', 'WARRANTY_COST']].copy()
        cluster_predictions['predicted_warranty_cost'] = y_pred
        cluster_predictions['cluster'] = cluster_id
        cluster_predictions['model_mae'] = model_info['mae']
        cluster_predictions['prediction_error'] = np.abs(cluster_predictions['WARRANTY_COST'] - y_pred)
        
        predictions.append(cluster_predictions)
    
    if predictions:
        return pd.concat(predictions, ignore_index=True)
    else:
        return pd.DataFrame()

# =============================================================================
# 12. EXAMPLE PREDICTIONS AND RESULTS
# =============================================================================

print(f"\nüéØ Making sample predictions...")

# Sample test data
test_data = df_model.sample(20, random_state=42).copy()
predictions = predict_warranty_cost(test_data, cluster_models, cluster_df, scaler, feature_cols)

if not predictions.empty:
    print(f"\nüìã Sample Predictions:")
    display_cols = ['BIKE_GROUPED', 'WARRANTY_COST', 'predicted_warranty_cost', 'cluster', 'prediction_error']
    print(predictions[display_cols].head(10))
    
    sample_mae = mean_absolute_error(predictions['WARRANTY_COST'], predictions['predicted_warranty_cost'])
    sample_mape = np.mean(np.abs((predictions['WARRANTY_COST'] - predictions['predicted_warranty_cost']) / predictions['WARRANTY_COST'])) * 100
    
    print(f"\nüéØ Sample Prediction Performance:")
    print(f"MAE: {sample_mae:.0f}")
    print(f"MAPE: {sample_mape:.1f}%")

# =============================================================================
# 13. FEATURE IMPORTANCE ANALYSIS
# =============================================================================

print(f"\nüîç Feature Importance Analysis:")

# Get feature importance from each cluster model
feature_importance_data = []
for cluster_id, model_info in cluster_models.items():
    model = model_info['model']
    
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        for i, feature in enumerate(feature_cols):
            feature_importance_data.append({
                'cluster': cluster_id,
                'feature': feature,
                'importance': importances[i]
            })

if feature_importance_data:
    importance_df = pd.DataFrame(feature_importance_data)
    
    # Top features overall
    overall_importance = importance_df.groupby('feature')['importance'].mean().sort_values(ascending=False)
    print(f"\nüèÜ Top 10 Most Important Features:")
    print(overall_importance.head(10))

print(f"\n" + "="*60)
print("üéâ WARRANTY FORECASTING CLUSTERING PIPELINE COMPLETED!")
print("="*60)
print(f"‚úÖ Models trained for {len(cluster_models)} clusters")
print(f"üìä Improvement over baseline: {improvement:.1f}%")
print(f"üöÄ Pipeline ready for production use!")
