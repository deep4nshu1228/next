# Fixed Warranty Forecasting with Proper Data Cleaning
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, mean_absolute_error, mean_squared_error
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import warnings
warnings.filterwarnings('ignore')

print("âœ… Libraries imported - Starting fixed warranty forecasting pipeline")

# =============================================================================
# 1. DATA PREPARATION WITH PROPER CLEANING
# =============================================================================

def clean_data_for_scaling(df):
    """Clean data to handle inf, -inf, and NaN values"""
    print("ðŸ§¹ Cleaning data for scaling...")
    
    # Replace infinite values with NaN first
    df_clean = df.replace([np.inf, -np.inf], np.nan)
    
    # Check for problematic values
    nan_count = df_clean.isnull().sum().sum()
    print(f"ðŸ“Š Found {nan_count} NaN/inf values to handle")
    
    # Handle NaN values by filling with appropriate values
    numeric_columns = df_clean.select_dtypes(include=[np.number]).columns
    
    for col in numeric_columns:
        if df_clean[col].isnull().any():
            # For ratio/percentage columns, fill with median
            if 'ratio' in col.lower() or 'per_unit' in col.lower():
                fill_value = df_clean[col].median()
            # For count/volume columns, fill with mean
            elif 'volume' in col.lower() or 'cost' in col.lower():
                fill_value = df_clean[col].mean()
            # For trend/variance columns, fill with 0
            elif 'trend' in col.lower() or 'variance' in col.lower():
                fill_value = 0
            else:
                fill_value = df_clean[col].median()
            
            # If median/mean is also NaN, use 0
            if pd.isna(fill_value):
                fill_value = 0
                
            df_clean[col] = df_clean[col].fillna(fill_value)
    
    return df_clean

# Sample data creation with potential infinite values
np.random.seed(42)
bike_models = [f'Model_{i:02d}' for i in range(1, 25)]
months = pd.date_range('2022-01', '2024-12', freq='M')

sample_data = []
for model in bike_models:
    base_cost = np.random.uniform(50, 200)
    seasonality = np.random.uniform(0.8, 1.2, 12)
    
    for month in months:
        dispatch_month = month - pd.DateOffset(months=np.random.randint(1, 13))
        seasonal_factor = seasonality[month.month - 1]
        volume = np.random.randint(50, 2000)  # Avoid very small volumes
        age = max(1, (month - dispatch_month).days // 30)  # Avoid zero age
        
        warranty_cost = (base_cost * volume * seasonal_factor * 
                        (1 + age * 0.02) * np.random.uniform(0.7, 1.3))
        
        sample_data.append({
            'BIKE_GROUPED': model,
            'DISPATCH_MONTH': dispatch_month.strftime('%m-%Y'),
            'WARRANTY_MONTH': month.strftime('%m-%Y'),
            'AGE': age,
            'VOLUME': volume,
            'WARRANTY_COST': warranty_cost,
        })

df = pd.DataFrame(sample_data)

# Safe calculation of COST_PER_UNIT to avoid division by zero
df['COST_PER_UNIT'] = np.where(df['VOLUME'] > 0, 
                               df['WARRANTY_COST'] / df['VOLUME'], 
                               0)

df['DISPATCH_DATE'] = pd.to_datetime(df['DISPATCH_MONTH'], format='%m-%Y')
df['WARRANTY_DATE'] = pd.to_datetime(df['WARRANTY_MONTH'], format='%m-%Y')

print(f"âœ… Data prepared: {df.shape[0]} records, {df['BIKE_GROUPED'].nunique()} models")

# =============================================================================
# 2. SAFE FEATURE EXTRACTION WITH INF/NAN HANDLING
# =============================================================================

warranty_ts = df.pivot_table(
    index='WARRANTY_DATE',
    columns='BIKE_GROUPED',
    values='WARRANTY_COST',
    aggfunc='sum',
    fill_value=0
)

def extract_essential_features_safe(ts_data, df_original):
    """Extract features with proper handling of infinite values"""
    features = pd.DataFrame(index=ts_data.columns)
    
    for model in ts_data.columns:
        try:
            series = ts_data[model]
            model_data = df_original[df_original['BIKE_GROUPED'] == model]
            
            # Safe calculations with checks
            # 1. COST PATTERNS
            features.loc[model, 'avg_warranty_cost'] = series.mean() if len(series) > 0 else 0
            features.loc[model, 'total_warranty_cost'] = series.sum()
            features.loc[model, 'cost_volatility'] = series.std() if len(series) > 1 else 0
            
            # Safe cost per unit calculation
            cpu_mean = model_data['COST_PER_UNIT'].mean()
            features.loc[model, 'avg_cost_per_unit'] = cpu_mean if pd.notna(cpu_mean) else 0
            
            # 2. SEASONALITY PATTERNS (with safe calculations)
            if len(series) > 0:
                monthly_avg = series.groupby(series.index.month).mean()
                monthly_avg = monthly_avg.replace([np.inf, -np.inf], np.nan).fillna(0)
                
                features.loc[model, 'seasonal_variance'] = monthly_avg.std() if len(monthly_avg) > 1 else 0
                features.loc[model, 'peak_month'] = monthly_avg.idxmax() if len(monthly_avg) > 0 else 1
                
                # Safe ratio calculation
                avg_val = monthly_avg.mean()
                max_val = monthly_avg.max()
                if avg_val > 0 and pd.notna(avg_val):
                    features.loc[model, 'peak_to_avg_ratio'] = max_val / avg_val
                else:
                    features.loc[model, 'peak_to_avg_ratio'] = 1
            else:
                features.loc[model, 'seasonal_variance'] = 0
                features.loc[model, 'peak_month'] = 1
                features.loc[model, 'peak_to_avg_ratio'] = 1
            
            # 3. TREND ANALYSIS (safe)
            if len(series) > 2:
                try:
                    trend_coef = np.polyfit(range(len(series)), series.values, 1)[0]
                    features.loc[model, 'cost_trend'] = trend_coef if pd.notna(trend_coef) else 0
                except:
                    features.loc[model, 'cost_trend'] = 0
            else:
                features.loc[model, 'cost_trend'] = 0
            
            # 4. VOLUME CHARACTERISTICS (safe)
            vol_mean = model_data['VOLUME'].mean()
            vol_std = model_data['VOLUME'].std()
            features.loc[model, 'avg_volume'] = vol_mean if pd.notna(vol_mean) else 0
            features.loc[model, 'volume_volatility'] = vol_std if pd.notna(vol_std) else 0
            
            # 5. AGE PATTERNS (safe)
            age_mean = model_data['AGE'].mean()
            age_max = model_data['AGE'].max()
            features.loc[model, 'avg_age'] = age_mean if pd.notna(age_mean) else 0
            features.loc[model, 'max_age'] = age_max if pd.notna(age_max) else 0
            
        except Exception as e:
            print(f"âš ï¸ Error processing model {model}: {e}")
            # Fill with safe defaults
            for col in ['avg_warranty_cost', 'total_warranty_cost', 'cost_volatility', 
                       'avg_cost_per_unit', 'seasonal_variance', 'peak_month', 
                       'peak_to_avg_ratio', 'cost_trend', 'avg_volume', 
                       'volume_volatility', 'avg_age', 'max_age']:
                if col not in features.columns:
                    features[col] = 0
                features.loc[model, col] = 0 if col != 'peak_month' else 1
    
    # Clean the features dataframe
    features = features.replace([np.inf, -np.inf], np.nan).fillna(0)
    
    return features

# Extract features safely
essential_features = extract_essential_features_safe(warranty_ts, df)
print(f"âœ… Essential features extracted safely: {essential_features.shape[1]} features")

# =============================================================================
# 3. SAFE SCALING WITH VERIFICATION
# =============================================================================

def safe_scaling(features_df):
    """Perform scaling with proper data verification"""
    print("ðŸ” Verifying data before scaling...")
    
    # Clean the features
    features_clean = clean_data_for_scaling(features_df)
    
    # Verify no infinite or very large values remain
    inf_check = np.isinf(features_clean.values).any()
    nan_check = np.isnan(features_clean.values).any()
    large_check = (np.abs(features_clean.values) > 1e10).any()
    
    print(f"ðŸ“Š Data check - Infinite: {inf_check}, NaN: {nan_check}, Very large: {large_check}")
    
    if inf_check or nan_check or large_check:
        print("âš ï¸ Found problematic values, applying additional cleaning...")
        
        # Cap extremely large values
        numeric_cols = features_clean.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            col_data = features_clean[col]
            
            # Cap at 99.9th percentile for very large values
            upper_cap = col_data.quantile(0.999)
            lower_cap = col_data.quantile(0.001)
            
            if pd.notna(upper_cap) and pd.notna(lower_cap):
                features_clean[col] = np.clip(col_data, lower_cap, upper_cap)
    
    # Final verification
    features_clean = features_clean.fillna(0).replace([np.inf, -np.inf], 0)
    
    # Scale the features
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features_clean)
    
    print("âœ… Scaling completed successfully!")
    return features_scaled, scaler, features_clean

# Perform safe scaling
features_scaled, scaler, features_clean = safe_scaling(essential_features)

# =============================================================================
# 4. CLUSTERING WITH CLEANED DATA
# =============================================================================

print("ðŸ” Finding optimal clusters...")
k_range = range(2, 8)
silhouette_scores = []

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)
    labels = kmeans.fit_predict(features_scaled)
    sil_score = silhouette_score(features_scaled, labels)
    silhouette_scores.append(sil_score)

optimal_k = k_range[np.argmax(silhouette_scores)]
print(f"âœ… Optimal clusters: {optimal_k}")

# Final clustering
final_clustering = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')
cluster_labels = final_clustering.fit_predict(features_scaled)

cluster_df = pd.DataFrame({
    'bike_model': features_clean.index,
    'cluster': cluster_labels
})

print("ðŸ“Š Cluster distribution:")
print(cluster_df['cluster'].value_counts().sort_index())

# =============================================================================
# 5. SAFE MODELING FEATURES
# =============================================================================

def create_safe_modeling_features(df_input):
    """Create modeling features with safe calculations"""
    df_model = df_input.copy()
    
    # Time features (safe)
    df_model['warranty_month'] = df_model['WARRANTY_DATE'].dt.month
    df_model['warranty_quarter'] = df_model['WARRANTY_DATE'].dt.quarter
    df_model['dispatch_month'] = df_model['DISPATCH_DATE'].dt.month
    
    # Cyclical encoding (always finite)
    df_model['warranty_month_sin'] = np.sin(2 * np.pi * df_model['warranty_month'] / 12)
    df_model['warranty_month_cos'] = np.cos(2 * np.pi * df_model['warranty_month'] / 12)
    
    # Safe transformations
    df_model['log_volume'] = np.log1p(np.maximum(df_model['VOLUME'], 0))  # Ensure positive
    df_model['age_squared'] = np.power(np.maximum(df_model['AGE'], 0), 2)  # Ensure positive
    
    # Safe lag calculation
    df_model['warranty_lag'] = np.maximum(
        (df_model['WARRANTY_DATE'] - df_model['DISPATCH_DATE']).dt.days / 30.44, 
        0
    )
    
    # Safe interaction
    df_model['age_volume_interaction'] = df_model['AGE'] * df_model['log_volume']
    
    # Simple lag feature
    df_model = df_model.sort_values(['BIKE_GROUPED', 'WARRANTY_DATE'])
    df_model['warranty_cost_lag1'] = df_model.groupby('BIKE_GROUPED')['WARRANTY_COST'].shift(1).fillna(0)
    
    # Clean all features
    numeric_cols = df_model.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        df_model[col] = df_model[col].replace([np.inf, -np.inf], np.nan)
        df_model[col] = df_model[col].fillna(df_model[col].median() if df_model[col].count() > 0 else 0)
    
    return df_model

# Apply safe feature creation
df_clustered = df.merge(cluster_df, left_on='BIKE_GROUPED', right_on='bike_model', how='left')
df_model = create_safe_modeling_features(df_clustered)

# Essential feature columns
essential_model_features = [
    'warranty_month', 'warranty_quarter', 'dispatch_month',
    'warranty_month_sin', 'warranty_month_cos',
    'AGE', 'age_squared', 'log_volume', 'VOLUME',
    'warranty_lag', 'age_volume_interaction',
    'warranty_cost_lag1', 'COST_PER_UNIT'
]

print(f"âœ… Safe modeling features created: {len(essential_model_features)}")

# =============================================================================
# 6. MODEL TRAINING WITH SAFE DATA
# =============================================================================

print("ðŸš€ Training models with cleaned data...")

cluster_models = {}
model_performance = []

for cluster_id in sorted(df_model['cluster'].unique()):
    cluster_data = df_model[df_model['cluster'] == cluster_id].copy()
    
    if len(cluster_data) < 30:
        continue
    
    # Safe feature preparation
    X = cluster_data[essential_model_features].copy()
    
    # Final cleaning of features
    X = X.replace([np.inf, -np.inf], np.nan).fillna(0)
    
    # Verify no problematic values
    if np.isinf(X.values).any() or np.isnan(X.values).any():
        print(f"âš ï¸ Found problematic values in cluster {cluster_id}, applying additional cleaning...")
        X = X.fillna(0).replace([np.inf, -np.inf], 0)
    
    y = cluster_data['WARRANTY_COST'].copy()
    
    # Split and train
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42
    )
    
    model = lgb.LGBMRegressor(
        n_estimators=300,
        learning_rate=0.1,
        max_depth=6,
        num_leaves=31,
        random_state=42,
        verbosity=-1
    )
    
    model.fit(X_train, y_train)
    
    # Evaluate
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / np.maximum(y_test, 1))) * 100  # Safe MAPE
    
    cluster_models[cluster_id] = {
        'model': model,
        'mae': mae,
        'mape': mape,
        'n_samples': len(cluster_data)
    }
    
    model_performance.append({
        'cluster': cluster_id,
        'n_samples': len(cluster_data),
        'mae': mae,
        'mape': mape
    })
    
    print(f"âœ… Cluster {cluster_id}: MAE={mae:.0f}, MAPE={mape:.1f}%, Samples={len(cluster_data)}")

# Calculate performance comparison
if model_performance:
    performance_df = pd.DataFrame(model_performance)
    clustered_weighted_mae = np.average(performance_df['mae'], weights=performance_df['n_samples'])
    
    print(f"\nðŸŽ¯ Clustered Model Weighted MAE: {clustered_weighted_mae:.0f}")
    print("âœ… Data cleaning successful - no more infinite/NaN value errors!")
else:
    print("âš ï¸ No models trained - check data quality")

print(f"\n" + "="*60)
print("ðŸŽ‰ FIXED WARRANTY FORECASTING PIPELINE COMPLETED!")
print("="*60)
print("âœ… All infinite and NaN values handled properly")
print("ðŸ”§ Pipeline ready for production use!")
