# Streamlined Warranty Forecasting with Essential Features Only
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, mean_absolute_error, mean_squared_error
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import warnings
warnings.filterwarnings('ignore')

print("‚úÖ Libraries imported - Starting streamlined warranty forecasting pipeline")

# =============================================================================
# 1. DATA PREPARATION WITH SAMPLE DATA
# =============================================================================

np.random.seed(42)
bike_models = [f'Model_{i:02d}' for i in range(1, 25)]
months = pd.date_range('2022-01', '2024-12', freq='M')

sample_data = []
for model in bike_models:
    base_cost = np.random.uniform(50, 200)
    seasonality = np.random.uniform(0.8, 1.2, 12)
    
    for month in months:
        dispatch_month = month - pd.DateOffset(months=np.random.randint(1, 13))
        seasonal_factor = seasonality[month.month - 1]
        volume = np.random.randint(100, 2000)
        age = (month - dispatch_month).days // 30
        
        warranty_cost = (base_cost * volume * seasonal_factor * 
                        (1 + age * 0.02) * np.random.uniform(0.7, 1.3))
        
        sample_data.append({
            'BIKE_GROUPED': model,
            'DISPATCH_MONTH': dispatch_month.strftime('%m-%Y'),
            'WARRANTY_MONTH': month.strftime('%m-%Y'),
            'AGE': age,
            'VOLUME': volume,
            'WARRANTY_COST': warranty_cost,
        })

df = pd.DataFrame(sample_data)
df['COST_PER_UNIT'] = df['WARRANTY_COST'] / df['VOLUME']
df['DISPATCH_DATE'] = pd.to_datetime(df['DISPATCH_MONTH'], format='%m-%Y')
df['WARRANTY_DATE'] = pd.to_datetime(df['WARRANTY_MONTH'], format='%m-%Y')

print(f"‚úÖ Data prepared: {df.shape[0]} records, {df['BIKE_GROUPED'].nunique()} models")

# =============================================================================
# 2. ESSENTIAL TIME SERIES FEATURE EXTRACTION
# =============================================================================

# Create time series pivot
warranty_ts = df.pivot_table(
    index='WARRANTY_DATE',
    columns='BIKE_GROUPED',
    values='WARRANTY_COST',
    aggfunc='sum',
    fill_value=0
)

def extract_essential_features(ts_data, df_original):
    """Extract only the most critical features for warranty clustering"""
    features = pd.DataFrame(index=ts_data.columns)
    
    for model in ts_data.columns:
        series = ts_data[model]
        model_data = df_original[df_original['BIKE_GROUPED'] == model]
        
        # 1. COST PATTERNS (Most Important)
        features.loc[model, 'avg_warranty_cost'] = series.mean()
        features.loc[model, 'total_warranty_cost'] = series.sum()
        features.loc[model, 'cost_volatility'] = series.std()
        features.loc[model, 'avg_cost_per_unit'] = model_data['COST_PER_UNIT'].mean()
        
        # 2. SEASONALITY PATTERNS (Critical for clustering)
        monthly_avg = series.groupby(series.index.month).mean()
        features.loc[model, 'seasonal_variance'] = monthly_avg.std()
        features.loc[model, 'peak_month'] = monthly_avg.idxmax()
        features.loc[model, 'peak_to_avg_ratio'] = (monthly_avg.max() / monthly_avg.mean() 
                                                   if monthly_avg.mean() > 0 else 1)
        
        # 3. TREND ANALYSIS (Essential for forecasting)
        if len(series) > 2:
            features.loc[model, 'cost_trend'] = np.polyfit(range(len(series)), series.values, 1)[0]
        else:
            features.loc[model, 'cost_trend'] = 0
            
        # 4. VOLUME CHARACTERISTICS (Business Critical)
        features.loc[model, 'avg_volume'] = model_data['VOLUME'].mean()
        features.loc[model, 'volume_volatility'] = model_data['VOLUME'].std()
        
        # 5. AGE PATTERNS (Warranty Specific)
        features.loc[model, 'avg_age'] = model_data['AGE'].mean()
        features.loc[model, 'max_age'] = model_data['AGE'].max()
        
    return features.fillna(0)

# Extract essential features
essential_features = extract_essential_features(warranty_ts, df)
print(f"‚úÖ Essential features extracted: {essential_features.shape[1]} features")
print("üîç Key features:", list(essential_features.columns))

# =============================================================================
# 3. CLUSTERING WITH ESSENTIAL FEATURES
# =============================================================================

# Scale features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(essential_features)

# Find optimal clusters (simplified)
print("üîç Finding optimal clusters...")
k_range = range(2, 8)
silhouette_scores = []

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)
    labels = kmeans.fit_predict(features_scaled)
    sil_score = silhouette_score(features_scaled, labels)
    silhouette_scores.append(sil_score)

optimal_k = k_range[np.argmax(silhouette_scores)]
print(f"‚úÖ Optimal clusters: {optimal_k}")

# Perform final clustering
final_clustering = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')
cluster_labels = final_clustering.fit_predict(features_scaled)

cluster_df = pd.DataFrame({
    'bike_model': essential_features.index,
    'cluster': cluster_labels
})

print("üìä Cluster distribution:")
print(cluster_df['cluster'].value_counts().sort_index())

# =============================================================================
# 4. ESSENTIAL MODELING FEATURES ONLY
# =============================================================================

def create_essential_modeling_features(df_input):
    """Create only the most important modeling features"""
    df_model = df_input.copy()
    
    # CORE FEATURES ONLY
    # 1. Time features (essential for seasonality)
    df_model['warranty_month'] = df_model['WARRANTY_DATE'].dt.month
    df_model['warranty_quarter'] = df_model['WARRANTY_DATE'].dt.quarter
    df_model['dispatch_month'] = df_model['DISPATCH_DATE'].dt.month
    
    # 2. Cyclical encoding for seasonality (most important)
    df_model['warranty_month_sin'] = np.sin(2 * np.pi * df_model['warranty_month'] / 12)
    df_model['warranty_month_cos'] = np.cos(2 * np.pi * df_model['warranty_month'] / 12)
    
    # 3. Core business features
    df_model['log_volume'] = np.log1p(df_model['VOLUME'])
    df_model['age_squared'] = df_model['AGE'] ** 2
    df_model['warranty_lag'] = (df_model['WARRANTY_DATE'] - df_model['DISPATCH_DATE']).dt.days / 30.44
    
    # 4. Key interaction (most predictive)
    df_model['age_volume_interaction'] = df_model['AGE'] * np.log1p(df_model['VOLUME'])
    
    # 5. Simple lag feature (if enough data)
    df_model = df_model.sort_values(['BIKE_GROUPED', 'WARRANTY_DATE'])
    df_model['warranty_cost_lag1'] = df_model.groupby('BIKE_GROUPED')['WARRANTY_COST'].shift(1).fillna(0)
    
    return df_model

# Merge cluster info and create features
df_clustered = df.merge(cluster_df, left_on='BIKE_GROUPED', right_on='bike_model', how='left')
df_model = create_essential_modeling_features(df_clustered)

# Essential feature columns only
essential_model_features = [
    'WARRANTY_MONTH', 'warranty_quarter', 'dispatch_month',
    'warranty_month_sin', 'warranty_month_cos',
    'AGE', 'age_squared', 'log_volume', 'VOLUME',
    'warranty_lag', 'age_volume_interaction',
    'warranty_cost_lag1', 'COST_PER_UNIT'
]

print(f"‚úÖ Essential modeling features: {len(essential_model_features)}")

# =============================================================================
# 5. STREAMLINED MODEL TRAINING
# =============================================================================

print("üöÄ Training cluster-specific models...")

cluster_models = {}
model_performance = []

for cluster_id in sorted(df_model['cluster'].unique()):
    cluster_data = df_model[df_model['cluster'] == cluster_id].copy()
    
    if len(cluster_data) < 30:
        continue  # Skip clusters with insufficient data
    
    # Prepare data
    X = cluster_data[essential_model_features].fillna(0)
    y = cluster_data['WARRANTY_COST']
    
    # Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42
    )
    
    # Train LightGBM
    model = lgb.LGBMRegressor(
        n_estimators=300,
        learning_rate=0.1,
        max_depth=6,
        num_leaves=31,
        random_state=42,
        verbosity=-1
    )
    
    model.fit(X_train, y_train)
    
    # Evaluate
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    
    cluster_models[cluster_id] = {
        'model': model,
        'mae': mae,
        'mape': mape,
        'n_samples': len(cluster_data)
    }
    
    model_performance.append({
        'cluster': cluster_id,
        'n_samples': len(cluster_data),
        'mae': mae,
        'mape': mape
    })
    
    print(f"‚úÖ Cluster {cluster_id}: MAE={mae:.0f}, MAPE={mape:.1f}%, Samples={len(cluster_data)}")

# =============================================================================
# 6. BASELINE COMPARISON
# =============================================================================

print("\nüîÑ Training baseline model...")
X_baseline = df_model[essential_model_features].fillna(0)
y_baseline = df_model['WARRANTY_COST']
X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(
    X_baseline, y_baseline, test_size=0.25, random_state=42
)

baseline_model = lgb.LGBMRegressor(
    n_estimators=300,
    learning_rate=0.1,
    max_depth=6,
    random_state=42,
    verbosity=-1
)
baseline_model.fit(X_train_base, y_train_base)
y_pred_baseline = baseline_model.predict(X_test_base)

baseline_mae = mean_absolute_error(y_test_base, y_pred_baseline)
baseline_mape = np.mean(np.abs((y_test_base - y_pred_baseline) / y_test_base)) * 100

# Calculate improvement
performance_df = pd.DataFrame(model_performance)
clustered_weighted_mae = np.average(performance_df['mae'], weights=performance_df['n_samples'])
improvement = ((baseline_mae - clustered_weighted_mae) / baseline_mae) * 100

print(f"üìà Baseline Model: MAE={baseline_mae:.0f}, MAPE={baseline_mape:.1f}%")
print(f"üéØ Clustered Model: MAE={clustered_weighted_mae:.0f}")
print(f"üöÄ Improvement: {improvement:.1f}%")

# =============================================================================
# 7. FEATURE IMPORTANCE ANALYSIS
# =============================================================================

print("\nüîç Most Important Features:")
feature_importance_dict = {}

for cluster_id, model_info in cluster_models.items():
    model = model_info['model']
    importances = model.feature_importances_
    
    for i, feature in enumerate(essential_model_features):
        if feature not in feature_importance_dict:
            feature_importance_dict[feature] = []
        feature_importance_dict[feature].append(importances[i])

# Calculate average importance across clusters
avg_importance = {feature: np.mean(scores) for feature, scores in feature_importance_dict.items()}
sorted_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)

print("üèÜ Top Features by Importance:")
for i, (feature, importance) in enumerate(sorted_features[:8], 1):
    print(f"{i:2d}. {feature:<25} {importance:.3f}")

# =============================================================================
# 8. STREAMLINED PREDICTION FUNCTION
# =============================================================================

def predict_warranty_cost_simple(new_data, cluster_models, cluster_df, essential_features):
    """Simplified prediction function"""
    # Add cluster info
    new_data_clustered = new_data.merge(
        cluster_df, left_on='BIKE_GROUPED', right_on='bike_model', how='left'
    )
    
    # Create features
    new_data_features = create_essential_modeling_features(new_data_clustered)
    
    predictions = []
    for cluster_id in cluster_models.keys():
        cluster_data = new_data_features[new_data_features['cluster'] == cluster_id].copy()
        
        if len(cluster_data) == 0:
            continue
            
        X = cluster_data[essential_features].fillna(0)
        y_pred = cluster_models[cluster_id]['model'].predict(X)
        
        cluster_predictions = cluster_data[['BIKE_GROUPED', 'WARRANTY_COST']].copy()
        cluster_predictions['predicted_warranty_cost'] = y_pred
        cluster_predictions['cluster'] = cluster_id
        
        predictions.append(cluster_predictions)
    
    return pd.concat(predictions, ignore_index=True) if predictions else pd.DataFrame()

# =============================================================================
# 9. SAMPLE PREDICTIONS AND VISUALIZATION
# =============================================================================

# Test predictions
test_data = df_model.sample(15, random_state=42)
predictions = predict_warranty_cost_simple(test_data, cluster_models, cluster_df, essential_model_features)

if not predictions.empty:
    print(f"\nüìã Sample Predictions:")
    sample_results = predictions[['BIKE_GROUPED', 'WARRANTY_COST', 'predicted_warranty_cost', 'cluster']].head(8)
    print(sample_results)
    
    sample_mae = mean_absolute_error(predictions['WARRANTY_COST'], predictions['predicted_warranty_cost'])
    print(f"\nüéØ Sample MAE: {sample_mae:.0f}")

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Plot 1: Cluster characteristics
cluster_summary = essential_features.groupby(cluster_df.set_index('bike_model')['cluster']).mean()
cluster_summary['avg_warranty_cost'].plot(kind='bar', ax=axes[0,0], color='skyblue')
axes[0,0].set_title('Average Warranty Cost by Cluster')
axes[0,0].set_ylabel('Warranty Cost')

# Plot 2: Feature importance
top_features = dict(sorted_features[:6])
axes[0,1].barh(list(top_features.keys()), list(top_features.values()), color='lightcoral')
axes[0,1].set_title('Top 6 Feature Importance')
axes[0,1].set_xlabel('Importance')

# Plot 3: Cluster distribution
cluster_df['cluster'].value_counts().sort_index().plot(kind='bar', ax=axes[1,0], color='lightgreen')
axes[1,0].set_title('Models per Cluster')
axes[1,0].set_ylabel('Number of Models')

# Plot 4: Model performance comparison
clusters = performance_df['cluster'].values
maes = performance_df['mae'].values
axes[1,1].bar(clusters, maes, color='gold', alpha=0.7, label='Cluster MAE')
axes[1,1].axhline(y=baseline_mae, color='red', linestyle='--', linewidth=2, label='Baseline MAE')
axes[1,1].set_title('Model Performance Comparison')
axes[1,1].set_xlabel('Cluster ID')
axes[1,1].set_ylabel('MAE')
axes[1,1].legend()

plt.tight_layout()
plt.show()

print(f"\n" + "="*60)
print("üéâ STREAMLINED WARRANTY FORECASTING COMPLETED!")
print("="*60)
print(f"‚úÖ Essential features used: {len(essential_model_features)}")
print(f"üìä Clusters created: {len(cluster_models)}")
print(f"üöÄ Performance improvement: {improvement:.1f}%")
print(f"üí° Key insight: Focus on seasonality, age, and volume interactions")
print(f"üîß Production ready with minimal complexity!")
