# Jupyter-ready end-to-end pipeline
import pandas as pd
import numpy as np
from datetime import timedelta

# -----------------------------
# CONFIGURATION
# -----------------------------
# Required columns in each input dataframe:
# MOBILE_NUMBER (string or int), LEAD_DATE (datetime-like), LEAD_ORIGIN (string), PROPENSITY_SCORE (float)
# Optionally: MODEL_NAME/SOURCE, etc.

# Lead origin weight rules for "additional sources" only
ADDITIONAL_WEIGHTS = {
    'walkin': 0.15,     # 15%
    'btl': 0.15,        # 15%
    'digital': 0.05,    # 5%
    'telephony': 0.05,  # 5%
}
# Note:
# - These weights are applied when those origins appear as additional sources
#   within the 90-day lookback for the same MOBILE_NUMBER but different LEAD_ORIGIN than the selected main record.
# - The final score weighting is normalized to sum to 100% per MOBILE_NUMBER by assigning
#   base_weight = 1 - sum(additional_weights_present). If additional_weights_present sum to 0.25,
#   base_weight = 0.75, thus base + extras = 1.0.

# 90-day window size
LOOKBACK_DAYS = 90

# -----------------------------
# SAMPLE INPUT (replace with actual loads)
# -----------------------------
# Replace these with real loads, e.g. pd.read_csv('file1.csv')
# Ensure LEAD_DATE is parsed to datetime
def make_df(seed):
    np.random.seed(seed)
    n = 8
    mobile = np.random.choice(['90001','90002','90003','90004'], size=n)
    dates = pd.to_datetime('2025-09-30') - pd.to_timedelta(np.random.randint(0,120,size=n), unit='D')
    origins = np.random.choice(['walkin','btl','digital','telephony'], size=n)
    score = np.round(np.random.rand(n), 4)
    model = np.random.choice(['modelA','modelB','modelC'], size=n)
    return pd.DataFrame({
        'MOBILE_NUMBER': mobile,
        'LEAD_DATE': dates,
        'LEAD_ORIGIN': origins,
        'PROPENSITY_SCORE': score,
        'MODEL_NAME': model
    })

df1 = make_df(1)
df2 = make_df(2)
df3 = make_df(3)

# -----------------------------
# INGEST + UNION
# -----------------------------
df = pd.concat([df1, df2, df3], ignore_index=True)

# Standardize LEAD_ORIGIN to lowercase to match weight keys
df['LEAD_ORIGIN'] = df['LEAD_ORIGIN'].str.strip().str.lower()

# Ensure LEAD_DATE is datetime
df['LEAD_DATE'] = pd.to_datetime(df['LEAD_DATE'])

# -----------------------------
# RESTRICT TO LAST 90 DAYS PER MOBILE_NUMBER
# -----------------------------
# Interpretation:
# - For each MOBILE_NUMBER, consider records within the latest 90 days relative to that MOBILE_NUMBERâ€™s max LEAD_DATE.
# - If a global 90-day window is needed (e.g., relative to "today"), replace anchor_date with pd.Timestamp('today').normalize()
anchor = df.groupby('MOBILE_NUMBER', as_index=False)['LEAD_DATE'].max().rename(columns={'LEAD_DATE':'MAX_DATE'})
df = df.merge(anchor, on='MOBILE_NUMBER', how='left')
df = df[df['LEAD_DATE'] >= (df['MAX_DATE'] - pd.to_timedelta(LOOKBACK_DAYS, unit='D'))].copy()
df.drop(columns=['MAX_DATE'], inplace=True)

# -----------------------------
# SELECT MAIN RECORD PER MOBILE_NUMBER
# -----------------------------
# Rule:
# - Choose record with maximum PROPENSITY_SCORE
# - Ties broken by latest LEAD_DATE
# - If still ties, keep the first occurrence deterministically via stable sort on index
df_sorted = df.sort_values(
    by=['MOBILE_NUMBER','PROPENSITY_SCORE','LEAD_DATE'],
    ascending=[True, False, False]
)
# Keep first row per MOBILE_NUMBER (max propensity, then latest date)
main_idx = df_sorted.groupby('MOBILE_NUMBER', as_index=False).head(1).index
df['IS_MAIN'] = False
df.loc[main_idx, 'IS_MAIN'] = True

# -----------------------------
# BUILD ADDITIONAL SOURCES PER MOBILE_NUMBER
# -----------------------------
# Additional sources are other distinct LEAD_ORIGIN values present in the 90-day window for the same MOBILE_NUMBER,
# excluding the main record's LEAD_ORIGIN.
main = df[df['IS_MAIN']].copy()
others = df[~df['IS_MAIN']].copy()

# Compute additional distinct origins per MOBILE_NUMBER, excluding the main origin
# Step 1: map MOBILE_NUMBER -> main_origin
main_origin_map = main.set_index('MOBILE_NUMBER')['LEAD_ORIGIN'].to_dict()

# Step 2: collect distinct origins in others
additional_origins = (
    others.groupby('MOBILE_NUMBER')['LEAD_ORIGIN']
    .apply(lambda s: sorted(set(s)))
    .reindex(main['MOBILE_NUMBER'])
)

# Step 3: exclude main origin from additional list per MOBILE_NUMBER
def exclude_main_origin(mobile, extra_list):
    if pd.isna(extra_list):
        return []
    mo = main_origin_map.get(mobile, None)
    return [o for o in extra_list if o != mo]

main['ADDITIONAL_ORIGINS'] = [
    exclude_main_origin(mob, additional_origins.loc[mob] if mob in additional_origins.index else [])
    for mob in main['MOBILE_NUMBER']
]

# -----------------------------
# DYNAMIC WEIGHTING
# -----------------------------
# For each MOBILE_NUMBER:
# - Sum additional weights among ADDITIONAL_ORIGINS using ADDITIONAL_WEIGHTS
# - Cap sum at < 1.0; if sum >= 1.0, renormalize additional weights proportionally to sum to 1.0 - epsilon (to avoid zero base)
# - Base weight = 1.0 - sum(additional_weights_present)
# Final propensity = base_weight * main.PROPENSITY_SCORE
# Optional: provide per-origin weight allocation breakdown.

def compute_weight_allocation(additional_list):
    # gather weights for present additional sources
    weights = {k: ADDITIONAL_WEIGHTS.get(k, 0.0) for k in additional_list}
    add_sum = sum(weights.values())
    # If add_sum >= 1.0, scale down proportionally to 0.95 so base gets 0.05 minimal
    # You may choose a different policy; here we maintain feasibility.
    if add_sum >= 1.0 and add_sum > 0:
        factor = 0.95 / add_sum
        weights = {k: v * factor for k, v in weights.items()}
        add_sum = sum(weights.values())
    base = 1.0 - add_sum
    return base, weights, add_sum

allocations = main['ADDITIONAL_ORIGINS'].apply(compute_weight_allocation)
main['BASE_WEIGHT'] = allocations.apply(lambda x: x[0])
main['EXTRA_WEIGHTS_DICT'] = allocations.apply(lambda x: x[1])
main['EXTRA_WEIGHT_SUM'] = allocations.apply(lambda x: x[2])

# Final weighted propensity for the main record
main['FINAL_PROPENSITY'] = main['BASE_WEIGHT'] * main['PROPENSITY_SCORE']

# For transparency, also expand extra weights into columns for known origins
for origin_key in ['walkin','btl','digital','telephony']:
    main[f'EXTRA_WT_{origin_key}'] = main['EXTRA_WEIGHTS_DICT'].apply(lambda d: float(d.get(origin_key, 0.0)))

# Sanity check: base + sum(extra) should be ~1.0
main['TOTAL_WEIGHT'] = main['BASE_WEIGHT'] + main['EXTRA_WEIGHT_SUM']

# -----------------------------
# OPTIONAL: ATTACH COUNTS AND MOST RECENT DATE IN WINDOW
# -----------------------------
agg_window = (
    df.groupby('MOBILE_NUMBER')
      .agg(
          RECORDS_90D=('MOBILE_NUMBER','size'),
          LATEST_LEAD_DATE=('LEAD_DATE','max'),
          DISTINCT_ORIGINS_90D=('LEAD_ORIGIN', lambda s: sorted(set(s)))
      )
      .reset_index()
)

result = (
    main.merge(agg_window, on='MOBILE_NUMBER', how='left')
         .sort_values(['FINAL_PROPENSITY','LEAD_DATE'], ascending=[False, False])
         .reset_index(drop=True)
)

# Select and order columns for output
cols = [
    'MOBILE_NUMBER',
    'LEAD_DATE',
    'LEAD_ORIGIN',
    'PROPENSITY_SCORE',
    'FINAL_PROPENSITY',
    'BASE_WEIGHT',
    'EXTRA_WEIGHT_SUM',
    'EXTRA_WT_walkin',
    'EXTRA_WT_btl',
    'EXTRA_WT_digital',
    'EXTRA_WT_telephony',
    'ADDITIONAL_ORIGINS',
    'RECORDS_90D',
    'LATEST_LEAD_DATE',
    'DISTINCT_ORIGINS_90D',
]
existing_cols = [c for c in cols if c in result.columns]
result = result[existing_cols]

# Display final result
result
