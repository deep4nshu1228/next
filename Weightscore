import pandas as pd
import numpy as np

# Ensure datetime and normalized text
df = df.copy()
df["LEAD_DATE"] = pd.to_datetime(df["LEAD_DATE"])
df["SOURCE_CHANNEL"] = df["SOURCE_CHANNEL"].str.strip().str.title()
df["LEAD_ORIGIN"] = df["LEAD_ORIGIN"].astype(str).str.strip()

# Step A: per-mobile ordered histories (oldest -> newest)
hist = (
    df.sort_values(["MOBILE_NUM","LEAD_DATE"])
      .groupby("MOBILE_NUM", as_index=False)
      .agg(
        SOURCE_CHANNELS_ASC=("SOURCE_CHANNEL", lambda s: ",".join(s)),
        LEAD_ORIGINS_ASC=("LEAD_ORIGIN", lambda s: ",".join(s)),
        SOURCE_EVENT_DATES=("LEAD_DATE", lambda s: ",".join(s.dt.strftime("%Y-%m-%d")))
      )
)

# Step B: presence flags per mobile for weighting
presence = (
    df.assign(ch=df["SOURCE_CHANNEL"].str.lower())
      .groupby("MOBILE_NUM")["ch"]
      .agg(lambda s: set(s))
      .reset_index()
)

def weight_from_sources(src_set):
    w = 0.0
    if "walkin" in src_set:     # +0.15
        w += 0.15
    if "digital" in src_set:    # +0.05
        w += 0.05
    if "telephony" in src_set:  # +0.05
        w += 0.05
    return w

presence["SRC_WEIGHT"] = presence["ch"].apply(weight_from_sources).astype(float)
presence = presence.drop(columns=["ch"])

# Step C: max and 75/25 recompute of propensity
max_prop = (
    df.groupby("MOBILE_NUM", as_index=False)["LEAD_PROPENSITY_SCORE"]
      .max()
      .rename(columns={"LEAD_PROPENSITY_SCORE":"MAX_PROP"})
)

# recomputed score = 0.75*MAX + 0.25*MAX + channel-weight*MAX = MAX*(1.0 + weight)
# per directions: “take 75% of it and rest 25% will be added as weightage”
# Interpreted as base = 75% of MAX; add 25% of MAX plus channel weight on top.
# Equivalent simplified: score = MAX*(0.75 + 0.25 + weight) = MAX*(1 + weight)
# If the intent is base 0.75*MAX + weight_as_absolute (0.15 etc) not scaled, change formula accordingly.
scored = max_prop.merge(presence, on="MOBILE_NUM", how="left").fillna({"SRC_WEIGHT":0.0})
scored["LEAD_PROPENSITY_SCORE_NEW"] = scored["MAX_PROP"] * (1.0 + scored["SRC_WEIGHT"])

# Step D: pick latest row per mobile for “latest” fields
latest_idx = (
    df.sort_values(["MOBILE_NUM","LEAD_DATE"])
      .groupby("MOBILE_NUM", as_index=False)
      .tail(1)
      .set_index("MOBILE_NUM")
)

# Step E: first non-null EMAIL per mobile
email_first = (
    df.sort_values(["MOBILE_NUM","LEAD_DATE"])
      .assign(EMAIL_NONNULL=df["EMAIL_ID"].where(df["EMAIL_ID"].notna()))
      .groupby("MOBILE_NUM", as_index=False)
      .agg(EMAIL_ID=("EMAIL_NONNULL", lambda s: next((x for x in s if pd.notna(x)), np.nan)))
)

# Step F: other reductions
max_cols = (
    df.groupby("MOBILE_NUM", as_index=False)
      .agg(
        LEAD_CONVERSION_POTENTIAL=("LEAD_PROPENSITY_SCORE","max"),  # “maximum score one”
        ESTIMATED_PURCHASE_DATE=("ESTIMATED_PURCHASE_DATE","max"),
        AVG_FOLLOWUPS_TO_CONVERSION=("AVG_FOLLOWUPS_TO_CONVERSION","max")
      )
)

# Assemble final
final = (
    scored[["MOBILE_NUM","LEAD_PROPENSITY_SCORE_NEW","MAX_PROP"]]
      .merge(latest_idx[[
          "CUSTOMER_NAME","STATE","LEAD_DATE","ADOBE_ID",
          "ESTIMATED_CONVERSION_TIME"
      ]].reset_index(), on="MOBILE_NUM", how="left")
      .merge(hist, on="MOBILE_NUM", how="left")
      .merge(email_first, on="MOBILE_NUM", how="left")
      .merge(max_cols, on="MOBILE_NUM", how="left")
)

# Rename/compute requested columns
final = final.rename(columns={
    "LEAD_PROPENSITY_SCORE_NEW":"LEAD_PROPENSITY_SCORE",
    "MAX_PROP":"LEAD_CONVERSION_POTENTIAL",  # keep original max as “maximum score one”
    "SOURCE_CHANNELS_ASC":"SOURCE_CHANNEL",
    "LEAD_ORIGINS_ASC":"LEAD_ORIGIN"
})
final["INSERT_DATERTIME_UST"] = pd.Timestamp.utcnow().tz_localize("UTC")

# Order columns
final = final[[
    "CUSTOMER_NAME",
    "EMAIL_ID",
    "MOBILE_NUM",
    "STATE",
    "LEAD_DATE",
    "ADOBE_ID",
    "INSERT_DATERTIME_UST",
    "LEAD_PROPENSITY_SCORE",
    "LEAD_CONVERSION_POTENTIAL",
    "ESTIMATED_CONVERSION_TIME",
    "ESTIMATED_PURCHASE_DATE",
    "LEAD_ORIGIN",
    "SOURCE_CHANNEL",
    "SOURCE_EVENT_DATES",
    "AVG_FOLLOWUPS_TO_CONVERSION"
]]
