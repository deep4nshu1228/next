import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score
import matplotlib.pyplot as plt
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Define imbalance methods
imbalance_methods = {
    'none': None,
    'smote': SMOTE(random_state=42),
    'adasyn': ADASYN(random_state=42),
    'borderline_smote': BorderlineSMOTE(random_state=42),
    'random_undersampling': RandomUnderSampler(random_state=42)
}

# Assume df is your DataFrame with features and target 'Conversion_time_bucket'
# Encode target variable
le = LabelEncoder()
df['target_enc'] = le.fit_transform(df['Conversion_time_bucket'])

# Define features and target
X = df.drop(columns=['Conversion_time_bucket', 'target_enc', 'converted'])  # Adjust as needed to drop other target cols
y = df['target_enc']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42)

# Identify numeric and categorical columns
numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()

# Preprocessor
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

preprocessor = ColumnTransformer([
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

# Compute class weights for use when no sampler applied
class_weights_arr = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = {i: w for i, w in enumerate(class_weights_arr)}

results = {}

for method_name, sampler in imbalance_methods.items():
    print(f"\nRunning imbalance handling method: {method_name}")

    model = XGBClassifier(
        objective='multi:softmax',
        num_class=len(le.classes_),
        eval_metric='mlogloss',
        use_label_encoder=False,
        random_state=42
    )

    steps = [('preprocessor', preprocessor)]
    if sampler:
        steps.append(('sampler', sampler))
    steps.append(('classifier', model))

    pipeline = ImbPipeline(steps=steps)

    try:
        if method_name == 'none':
            sample_weights = pd.Series(y_train).map(class_weights_dict).values
            pipeline.fit(X_train, y_train, classifier__sample_weight=sample_weights)
        else:
            pipeline.fit(X_train, y_train)

        y_pred = pipeline.predict(X_test)
        y_test_labels = le.inverse_transform(y_test)
        y_pred_labels = le.inverse_transform(y_pred)

        print(classification_report(y_test_labels, y_pred_labels))
        acc = accuracy_score(y_test_labels, y_pred_labels)
        f1 = f1_score(y_test_labels, y_pred_labels, average='weighted')

        results[method_name] = {'accuracy': acc, 'weighted_f1': f1}

        cm = confusion_matrix(y_test_labels, y_pred_labels, labels=le.classes_)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
        disp.plot(cmap='Blues', xticks_rotation=45)
        plt.title(f'Confusion Matrix - {method_name}')
        plt.show()

    except ValueError as e:
        print(f"Error with method '{method_name}': {e}")
        results[method_name] = {'error': str(e)}

# Print summary
print("\nSummary of imbalance handling methods:")
for method, metric in results.items():
    if 'error' in metric:
        print(f"{method}: Error - {metric['error']}")
    else:
        print(f"{method}: Accuracy={metric['accuracy']:.4f}, Weighted F1={metric['weighted_f1']:.4f}")
