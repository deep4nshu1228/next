import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score
import matplotlib.pyplot as plt
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Define imbalance methods
imbalance_methods = {
    'none': None,
    'smote': SMOTE(random_state=42),
    'adasyn': ADASYN(random_state=42),
    'borderline_smote': BorderlineSMOTE(random_state=42),
    'random_undersampling': RandomUnderSampler(random_state=42)
}

# Assume df is your DataFrame with features and target 'Conversion_time_bucket'
# Encode target variable
le = LabelEncoder()
df['target_enc'] = le.fit_transform(df['Conversion_time_bucket'])

# Define features and target
X = df.drop(columns=['Conversion_time_bucket', 'target_enc', 'converted'])  # Adjust as needed to drop other target cols
y = df['target_enc']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42)

# Identify numeric and categorical columns
numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()

# Preprocessor
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

preprocessor = ColumnTransformer([
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

# Compute class weights for use when no sampler applied
class_weights_arr = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = {i: w for i, w in enumerate(class_weights_arr)}

results = {}

for method_name, sampler in imbalance_methods.items():
    print(f"\nRunning imbalance handling method: {method_name}")

    model = XGBClassifier(
        objective='multi:softmax',
        num_class=len(le.classes_),
        eval_metric='mlogloss',
        use_label_encoder=False,
        random_state=42
    )

    steps = [('preprocessor', preprocessor)]
    if sampler:
        steps.append(('sampler', sampler))
    steps.append(('classifier', model))

    pipeline = ImbPipeline(steps=steps)

    try:
        if method_name == 'none':
            sample_weights = pd.Series(y_train).map(class_weights_dict).values
            pipeline.fit(X_train, y_train, classifier__sample_weight=sample_weights)
        else:
            pipeline.fit(X_train, y_train)

        y_pred = pipeline.predict(X_test)
        y_test_labels = le.inverse_transform(y_test)
        y_pred_labels = le.inverse_transform(y_pred)

        print(classification_report(y_test_labels, y_pred_labels))
        acc = accuracy_score(y_test_labels, y_pred_labels)
        f1 = f1_score(y_test_labels, y_pred_labels, average='weighted')

        results[method_name] = {'accuracy': acc, 'weighted_f1': f1}

        cm = confusion_matrix(y_test_labels, y_pred_labels, labels=le.classes_)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
        disp.plot(cmap='Blues', xticks_rotation=45)
        plt.title(f'Confusion Matrix - {method_name}')
        plt.show()

    except ValueError as e:
        print(f"Error with method '{method_name}': {e}")
        results[method_name] = {'error': str(e)}

# Print summary
print("\nSummary of imbalance handling methods:")
for method, metric in results.items():
    if 'error' in metric:
        print(f"{method}: Error - {metric['error']}")
    else:
        print(f"{method}: Accuracy={metric['accuracy']:.4f}, Weighted F1={metric['weighted_f1']:.4f}")







######################## Multiple sampling and classifires ########################################

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Define sampling techniques
sampling_methods = {
    'none': None,
    'smote': SMOTE(random_state=42),
    'adasyn': ADASYN(random_state=42),
    'borderline_smote': BorderlineSMOTE(random_state=42),
    'random_undersampling': RandomUnderSampler(random_state=42)
}

# Define classifiers
classifiers = {
    'XGBoost': XGBClassifier(objective='multi:softmax', num_class=None, use_label_encoder=False, eval_metric='mlogloss', random_state=42),
    'LightGBM': LGBMClassifier(objective='multiclass', random_state=42),
    'CatBoost': CatBoostClassifier(loss_function='MultiClass', verbose=0, random_state=42),
    'RandomForest': RandomForestClassifier(random_state=42)
}

# Prepare your dataset (df) and label encode target
le = LabelEncoder()
df['target_enc'] = le.fit_transform(df['Conversion_time_bucket'])
X = df.drop(columns=['Conversion_time_bucket', 'target_enc', 'converted'])
y = df['target_enc']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Preprocess features (fit on train, transform train and test)
numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()

numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse=False)

preprocessor = ColumnTransformer([
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

X_train_proc = preprocessor.fit_transform(X_train)
X_test_proc = preprocessor.transform(X_test)

# Compute class weights for use with 'none' sampling
class_weights_arr = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = {i: w for i, w in enumerate(class_weights_arr)}

# Apply sampling techniques offline
sampled_datasets = {}

for name, sampler in sampling_methods.items():
    print(f"Sampling method: {name}")
    if sampler is None:
        sampled_datasets[name] = (X_train_proc, y_train.values)
    else:
        X_res, y_res = sampler.fit_resample(X_train_proc, y_train)
        sampled_datasets[name] = (X_res, y_res)

# Run classifiers on each sampled dataset
results = []

for samp_name, (X_samp, y_samp) in sampled_datasets.items():
    for clf_name, clf in classifiers.items():
        print(f"\nTraining classifier {clf_name} on sampling {samp_name}")

        # Adjust num_class param if exists
        if hasattr(clf, 'num_class'):
            clf.num_class = len(le.classes_)

        try:
            if samp_name == 'none':
                # use sample weights for imbalance correction
                sample_weights = pd.Series(y_samp).map(class_weights_dict).values
                clf.fit(X_samp, y_samp, sample_weight=sample_weights)
            else:
                clf.fit(X_samp, y_samp)

            y_pred = clf.predict(X_test_proc)

            y_test_labels = le.inverse_transform(y_test)
            y_pred_labels = le.inverse_transform(y_pred)

            print(classification_report(y_test_labels, y_pred_labels))

            acc = accuracy_score(y_test_labels, y_pred_labels)
            f1 = f1_score(y_test_labels, y_pred_labels, average='weighted')

            results.append({
                'sampling': samp_name,
                'classifier': clf_name,
                'accuracy': acc,
                'weighted_f1': f1
            })

            cm = confusion_matrix(y_test_labels, y_pred_labels, labels=le.classes_)
            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
            disp.plot(cmap='Blues', xticks_rotation=45)
            plt.title(f'Sampling: {samp_name} | Classifier: {clf_name}')
            plt.show()

        except Exception as e:
            print(f"Error with sampling {samp_name} and classifier {clf_name}: {e}")
            results.append({
                'sampling': samp_name,
                'classifier': clf_name,
                'error': str(e)
            })

# Summary
results_df = pd.DataFrame(results)
print("\nSummary of all runs:")
print(results_df)
