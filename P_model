# =============================================================================
# ENSEMBLE MODELING WITH POLARS (NO PANDAS) - MEMORY EFFICIENT
# XGBoost + LightGBM + CatBoost + Ridge for Demand Forecasting
# =============================================================================

import polars as pl
import numpy as np
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostRegressor
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.optimize import minimize
import joblib
import logging

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")

# =============================================================================
# STEP 1: Prepare Data (Pure Polars)
# =============================================================================

def prepare_ensemble_data_polars(df_polars, target_col='TOTAL_QUANTITY'):
    """
    Prepare data for ensemble models with proper categorical handling
    Returns both encoded and original data for different model types
    """
    
    # Identify categorical columns
    cat_features = ['CUSTOMER_EXTERNAL_CODE', 'SKU_ERP_CODE']
    
    # Define feature columns
    exclude_cols = [
        target_col, 'WEEK_START_DATE', 'WEEK_END_DATE',
        'ORDER_IDS', 'ORDER_DATES', 'y', 'y_log1p',
        'TOTAL_DLP_VALUE', 'TOTAL_BASE_PRICE', 'BASE_PRICE'
    ]
    
    all_feature_cols = [c for c in df_polars.columns if c not in exclude_cols]
    
    # Create encoded version (for XGBoost, LightGBM, Ridge)
    df_encoded = df_polars.clone()
    for col in cat_features:
        if col in df_encoded.columns:
            df_encoded = df_encoded.with_columns(
                pl.col(col).cast(pl.Categorical).to_physical().alias(col)
            )
    
    # Get features and target
    feature_cols = [c for c in all_feature_cols if c in df_encoded.columns]
    
    # Extract as numpy (ENSURE FLOAT TYPE)
    X_encoded = df_encoded.select(feature_cols).to_numpy().astype(np.float32)
    y = df_encoded.select(target_col).to_numpy().flatten().astype(np.float32)
    
    # Get categorical column indices
    cat_indices = [i for i, col in enumerate(feature_cols) if col in cat_features]
    
    # For CatBoost: Keep original categorical values as strings
    X_catboost = df_polars.select(feature_cols).to_numpy()
    
    # Convert categoricals to string for CatBoost
    for idx in cat_indices:
        X_catboost[:, idx] = X_catboost[:, idx].astype(str)
    
    logging.info(f"Encoded features: {X_encoded.shape}, dtype: {X_encoded.dtype}")
    logging.info(f"CatBoost features: {X_catboost.shape}")
    logging.info(f"Target: {y.shape}, dtype: {y.dtype}")
    logging.info(f"Categorical indices: {cat_indices}")
    
    return X_encoded, X_catboost, y, feature_cols, cat_indices

class PolarsEnsembleForecaster:
    """
    Fixed ensemble forecaster with proper data type handling
    """
    
    def __init__(self):
        self.models = {}
        self.weights = None
        self.feature_names = None
        self.cat_indices = None
        
    def train_xgboost(self, X_train, y_train, X_valid, y_valid):
        """Train XGBoost (uses encoded data)"""
        logging.info("\n" + "="*60)
        logging.info("Training XGBoost...")
        logging.info("="*60)
        
        params = {
            'objective': 'reg:squarederror',
            'eval_metric': 'rmse',
            'max_depth': 5,
            'learning_rate': 0.03,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'min_child_weight': 10,
            'reg_alpha': 1.0,
            'reg_lambda': 2.0,
            'tree_method': 'hist',
            'random_state': 42
        }
        
        # Ensure float32
        X_train = X_train.astype(np.float32)
        X_valid = X_valid.astype(np.float32)
        y_train = y_train.astype(np.float32)
        y_valid = y_valid.astype(np.float32)
        
        dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=self.feature_names)
        dvalid = xgb.DMatrix(X_valid, label=y_valid, feature_names=self.feature_names)
        
        model = xgb.train(
            params,
            dtrain,
            num_boost_round=600,
            evals=[(dtrain, 'train'), (dvalid, 'valid')],
            early_stopping_rounds=50,
            verbose_eval=100
        )
        
        valid_pred = model.predict(dvalid)
        valid_rmse = np.sqrt(mean_squared_error(y_valid, valid_pred))
        valid_r2 = r2_score(y_valid, valid_pred)
        
        logging.info(f"XGBoost - Valid RMSE: {valid_rmse:.4f}, Valid R²: {valid_r2:.4f}")
        
        self.models['xgboost'] = model
        return valid_pred
    
    def train_lightgbm(self, X_train, y_train, X_valid, y_valid):
        """Train LightGBM (uses encoded data)"""
        logging.info("\n" + "="*60)
        logging.info("Training LightGBM...")
        logging.info("="*60)
        
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'max_depth': 6,
            'learning_rate': 0.03,
            'num_leaves': 31,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'min_child_weight': 10,
            'lambda_l1': 1.0,
            'lambda_l2': 2.0,
            'verbose': -1,
            'random_state': 42
        }
        
        # Ensure float32
        X_train = X_train.astype(np.float32)
        X_valid = X_valid.astype(np.float32)
        y_train = y_train.astype(np.float32)
        y_valid = y_valid.astype(np.float32)
        
        lgb_train = lgb.Dataset(X_train, label=y_train, feature_name=self.feature_names)
        lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)
        
        model = lgb.train(
            params,
            lgb_train,
            num_boost_round=600,
            valid_sets=[lgb_train, lgb_valid],
            valid_names=['train', 'valid'],
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]
        )
        
        valid_pred = model.predict(X_valid)
        valid_rmse = np.sqrt(mean_squared_error(y_valid, valid_pred))
        valid_r2 = r2_score(y_valid, valid_pred)
        
        logging.info(f"LightGBM - Valid RMSE: {valid_rmse:.4f}, Valid R²: {valid_r2:.4f}")
        
        self.models['lightgbm'] = model
        return valid_pred
    
    def train_catboost(self, X_train_cat, y_train, X_valid_cat, y_valid, cat_indices):
        """Train CatBoost (uses original categorical data)"""
        logging.info("\n" + "="*60)
        logging.info("Training CatBoost...")
        logging.info("="*60)
        
        # CatBoost handles categoricals internally
        model = CatBoostRegressor(
            iterations=600,
            depth=5,
            learning_rate=0.03,
            l2_leaf_reg=3.0,
            subsample=0.8,
            rsm=0.8,
            cat_features=cat_indices if cat_indices else None,
            loss_function='RMSE',
            eval_metric='RMSE',
            random_seed=42,
            verbose=100,
            early_stopping_rounds=50
        )
        
        model.fit(
            X_train_cat, y_train,
            eval_set=(X_valid_cat, y_valid),
            use_best_model=True
        )
        
        valid_pred = model.predict(X_valid_cat)
        valid_rmse = np.sqrt(mean_squared_error(y_valid, valid_pred))
        valid_r2 = r2_score(y_valid, valid_pred)
        
        logging.info(f"CatBoost - Valid RMSE: {valid_rmse:.4f}, Valid R²: {valid_r2:.4f}")
        
        self.models['catboost'] = model
        return valid_pred
    
    def train_ridge(self, X_train, y_train, X_valid, y_valid):
        """Train Ridge (uses encoded data)"""
        logging.info("\n" + "="*60)
        logging.info("Training Ridge Regression...")
        logging.info("="*60)
        
        # Ensure float32
        X_train = X_train.astype(np.float32)
        X_valid = X_valid.astype(np.float32)
        y_train = y_train.astype(np.float32)
        y_valid = y_valid.astype(np.float32)
        
        # Cross-validate alpha
        alphas = [0.1, 1.0, 10.0, 50.0, 100.0]
        best_score = float('inf')
        best_alpha = None
        
        for alpha in alphas:
            model = Ridge(alpha=alpha, random_state=42)
            model.fit(X_train, y_train)
            pred = model.predict(X_valid)
            rmse = np.sqrt(mean_squared_error(y_valid, pred))
            
            if rmse < best_score:
                best_score = rmse
                best_alpha = alpha
        
        model = Ridge(alpha=best_alpha, random_state=42)
        model.fit(X_train, y_train)
        
        valid_pred = model.predict(X_valid)
        valid_rmse = np.sqrt(mean_squared_error(y_valid, valid_pred))
        valid_r2 = r2_score(y_valid, valid_pred)
        
        logging.info(f"Ridge (α={best_alpha}) - Valid RMSE: {valid_rmse:.4f}, Valid R²: {valid_r2:.4f}")
        
        self.models['ridge'] = model
        return valid_pred
    
    def fit(self, X_train_encoded, X_train_catboost, y_train, 
            X_valid_encoded, X_valid_catboost, y_valid, 
            feature_names, cat_indices=None):
        """
        Train all models with proper data types
        
        Args:
            X_train_encoded: Encoded features for XGBoost/LightGBM/Ridge
            X_train_catboost: Original categorical features for CatBoost
            y_train: Training target
            X_valid_encoded: Validation encoded features
            X_valid_catboost: Validation categorical features
            y_valid: Validation target
            feature_names: Feature names
            cat_indices: Categorical column indices
        """
        
        self.feature_names = feature_names
        self.cat_indices = cat_indices
        
        predictions = {}
        
        # Train models with appropriate data
        predictions['xgboost'] = self.train_xgboost(
            X_train_encoded, y_train, X_valid_encoded, y_valid
        )
        
        predictions['lightgbm'] = self.train_lightgbm(
            X_train_encoded, y_train, X_valid_encoded, y_valid
        )
        
        predictions['catboost'] = self.train_catboost(
            X_train_catboost, y_train, X_valid_catboost, y_valid, cat_indices
        )
        
        predictions['ridge'] = self.train_ridge(
            X_train_encoded, y_train, X_valid_encoded, y_valid
        )
        
        return predictions
    
    def optimize_weights(self, predictions, y_true):
        """Find optimal ensemble weights"""
        logging.info("\n" + "="*60)
        logging.info("Optimizing Ensemble Weights...")
        logging.info("="*60)
        
        pred_matrix = np.column_stack([predictions[name] for name in predictions.keys()])
        n_models = pred_matrix.shape[1]
        
        def objective(weights):
            weights = weights / weights.sum()
            ensemble_pred = pred_matrix @ weights
            return np.sqrt(mean_squared_error(y_true, ensemble_pred))
        
        result = minimize(
            objective,
            x0=np.ones(n_models) / n_models,
            method='SLSQP',
            bounds=[(0, 1)] * n_models,
            constraints={'type': 'eq', 'fun': lambda w: w.sum() - 1}
        )
        
        optimal_weights = result.x
        self.weights = dict(zip(predictions.keys(), optimal_weights))
        
        logging.info("\nOptimal Weights:")
        for name, weight in self.weights.items():
            logging.info(f"  {name}: {weight:.4f}")
        
        return optimal_weights
    
    def predict(self, X_encoded, X_catboost, method='weighted'):
        """
        Make predictions
        
        Args:
            X_encoded: Encoded features for XGBoost/LightGBM/Ridge
            X_catboost: Categorical features for CatBoost
            method: Ensemble method
        """
        
        predictions = {}
        
        if 'xgboost' in self.models:
            dmatrix = xgb.DMatrix(X_encoded.astype(np.float32), feature_names=self.feature_names)
            predictions['xgboost'] = self.models['xgboost'].predict(dmatrix)
        
        if 'lightgbm' in self.models:
            predictions['lightgbm'] = self.models['lightgbm'].predict(X_encoded.astype(np.float32))
        
        if 'catboost' in self.models:
            predictions['catboost'] = self.models['catboost'].predict(X_catboost)
        
        if 'ridge' in self.models:
            predictions['ridge'] = self.models['ridge'].predict(X_encoded.astype(np.float32))
        
        pred_matrix = np.column_stack([predictions[name] for name in predictions.keys()])
        
        if method == 'simple':
            return pred_matrix.mean(axis=1)
        elif method == 'weighted':
            weights = np.array([self.weights[name] for name in predictions.keys()])
            return pred_matrix @ weights
        elif method == 'median':
            return np.median(pred_matrix, axis=1)
    
    def save(self, filepath):
        """Save ensemble"""
        joblib.dump({
            'models': self.models,
            'weights': self.weights,
            'feature_names': self.feature_names,
            'cat_indices': self.cat_indices
        }, filepath)
        logging.info(f"✓ Ensemble saved to {filepath}")
    
    def load(self, filepath):
        """Load ensemble"""
        data = joblib.load(filepath)
        self.models = data['models']
        self.weights = data['weights']
        self.feature_names = data['feature_names']
        self.cat_indices = data.get('cat_indices', None)
        logging.info(f"✓ Ensemble loaded from {filepath}")

# =============================================================================
# STEP 3: Complete Training Pipeline (Pure Polars)
# =============================================================================

def train_ensemble_pipeline(train_df_polars, valid_df_polars, target_col='TOTAL_QUANTITY'):
    """
    Complete ensemble training pipeline with fixed data handling
    """
    
    logging.info("\n" + "="*80)
    logging.info("ENSEMBLE TRAINING PIPELINE (FIXED)")
    logging.info("="*80)
    
    # Prepare data (returns both encoded and categorical versions)
    logging.info("\nPreparing training data...")
    X_train_enc, X_train_cat, y_train, feature_names, cat_indices = prepare_ensemble_data_polars(
        train_df_polars, target_col
    )
    
    logging.info("Preparing validation data...")
    X_valid_enc, X_valid_cat, y_valid, _, _ = prepare_ensemble_data_polars(
        valid_df_polars, target_col
    )
    
    # Initialize ensemble
    ensemble = PolarsEnsembleForecaster()
    
    # Train all models (passing both data versions)
    logging.info("\n" + "="*80)
    logging.info("TRAINING MODELS")
    logging.info("="*80)
    
    predictions = ensemble.fit(
        X_train_enc, X_train_cat, y_train,
        X_valid_enc, X_valid_cat, y_valid,
        feature_names=feature_names,
        cat_indices=cat_indices
    )
    
    # Optimize weights
    optimal_weights = ensemble.optimize_weights(predictions, y_valid)
    
    # Evaluate
    evaluate_ensemble_polars(ensemble, predictions, y_valid)
    
    return ensemble, predictions, y_valid

# =============================================================================
# STEP 4: Evaluation (Using Polars for Results)
# =============================================================================

def evaluate_ensemble_polars(ensemble, predictions, y_valid):
    """
    Evaluate ensemble performance and store results in Polars DataFrame
    """
    
    logging.info("\n" + "="*60)
    logging.info("ENSEMBLE EVALUATION")
    logging.info("="*60)
    
    results = []
    
    # Individual models
    logging.info("\nIndividual Models:")
    for name, pred in predictions.items():
        rmse = np.sqrt(mean_squared_error(y_valid, pred))
        mae = mean_absolute_error(y_valid, pred)
        r2 = r2_score(y_valid, pred)
        
        mask = y_valid > 0
        mape = np.mean(np.abs((y_valid[mask] - pred[mask]) / y_valid[mask])) * 100 if mask.sum() > 0 else np.nan
        
        logging.info(f"{name:15s} - RMSE: {rmse:6.2f}, MAE: {mae:6.2f}, R²: {r2:6.4f}, MAPE: {mape:5.1f}%")
        
        results.append({
            'model': name,
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'mape': mape
        })
    
    # Ensemble methods
    for method_name, method in [('Simple Average', 'simple'), ('Weighted', 'weighted'), ('Median', 'median')]:
        X_valid_dummy = np.column_stack([predictions[name] for name in predictions.keys()])
        
        if method == 'weighted' and ensemble.weights is None:
            continue
            
        # For evaluation, we already have predictions, just combine them
        pred_matrix = np.column_stack([predictions[name] for name in predictions.keys()])
        
        if method == 'simple':
            ens_pred = pred_matrix.mean(axis=1)
        elif method == 'weighted':
            weights = np.array([ensemble.weights[name] for name in predictions.keys()])
            ens_pred = pred_matrix @ weights
        elif method == 'median':
            ens_pred = np.median(pred_matrix, axis=1)
        
        rmse = np.sqrt(mean_squared_error(y_valid, ens_pred))
        mae = mean_absolute_error(y_valid, ens_pred)
        r2 = r2_score(y_valid, ens_pred)
        
        mask = y_valid > 0
        mape = np.mean(np.abs((y_valid[mask] - ens_pred[mask]) / y_valid[mask])) * 100 if mask.sum() > 0 else np.nan
        
        logging.info(f"\n{method_name:15s} - RMSE: {rmse:6.2f}, MAE: {mae:6.2f}, R²: {r2:6.4f}, MAPE: {mape:5.1f}%")
        
        results.append({
            'model': f'ensemble_{method}',
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'mape': mape
        })
    
    # Convert results to Polars DataFrame
    results_df = pl.DataFrame(results)
    
    # Save results
    results_df.write_csv("models/ensemble_evaluation.csv")
    logging.info("\n✓ Evaluation results saved to models/ensemble_evaluation.csv")
    
    return results_df

# =============================================================================
# STEP 5: Prediction Analysis (Pure Polars)
# =============================================================================

def analyze_predictions_polars(predictions, y_valid, valid_df_polars):
    """
    Analyze predictions and create comparison DataFrame using Polars
    """
    
    logging.info("\n" + "="*60)
    logging.info("PREDICTION ANALYSIS")
    logging.info("="*60)
    
    # Create predictions DataFrame
    pred_data = {
        'actual': y_valid,
        'xgboost': predictions['xgboost'],
        'lightgbm': predictions['lightgbm'],
        'catboost': predictions['catboost'],
        'ridge': predictions['ridge']
    }
    
    # Weighted ensemble
    pred_matrix = np.column_stack([predictions[name] for name in predictions.keys()])
    # Assume equal weights if not optimized
    pred_data['ensemble_simple'] = pred_matrix.mean(axis=1)
    
    # Create Polars DataFrame
    results_df = pl.DataFrame(pred_data)
    
    # Add identifiers from original data
    if valid_df_polars.height == results_df.height:
        results_df = results_df.with_columns([
            valid_df_polars['CUSTOMER_EXTERNAL_CODE'],
            valid_df_polars['SKU_ERP_CODE'],
            valid_df_polars['WEEK_START_DATE']
        ])
    
    # Calculate errors
    results_df = results_df.with_columns([
        (pl.col('actual') - pl.col('ensemble_simple')).alias('error'),
        ((pl.col('actual') - pl.col('ensemble_simple')).abs()).alias('abs_error'),
        ((pl.col('actual') - pl.col('ensemble_simple')).abs() / (pl.col('actual') + 0.01) * 100).alias('pct_error')
    ])
    
    # Show sample
    logging.info("\nSample Predictions:")
    print(results_df.head(20))
    
    # Summary statistics
    logging.info("\nPrediction Statistics:")
    print(results_df.select([
        'actual', 'xgboost', 'lightgbm', 'catboost', 'ridge', 'ensemble_simple'
    ]).describe())
    
    # Save
    results_df.write_csv("models/ensemble_predictions_polars.csv")
    logging.info("\n✓ Predictions saved to models/ensemble_predictions_polars.csv")
    
    return results_df

# =============================================================================
# STEP 6: Usage Example (Complete Pipeline)
# =============================================================================

def main_polars_ensemble():
    """
    Complete example: Train ensemble using Snowpark/Polars data
    """
    
    # Load data from Snowpark (already Polars-compatible)
    # from snowflake.snowpark import Session
    # session = Session.builder.configs({...}).create()
    # train_df_snowpark = session.table("TRAIN_TABLE")
    # valid_df_snowpark = session.table("VALID_TABLE")
    
    # Convert Snowpark to Polars (or load directly)
    # train_df = train_df_snowpark.to_pandas()  # Then pl.from_pandas()
    # OR load parquet directly:
    
    train_df = pl.read_parquet("train.parquet")
    valid_df = pl.read_parquet("valid.parquet")
    
    logging.info(f"Train shape: {train_df.shape}")
    logging.info(f"Valid shape: {valid_df.shape}")
    
    # Train ensemble
    ensemble, predictions, y_valid = train_ensemble_pipeline(
        train_df,
        valid_df,
        target_col='TOTAL_QUANTITY'
    )
    
    # Analyze predictions
    results_df = analyze_predictions_polars(predictions, y_valid, valid_df)
    
    # Save ensemble
    ensemble.save("models/polars_ensemble.pkl")
    
    logging.info("\n" + "="*80)
    logging.info("✓ ENSEMBLE TRAINING COMPLETE")
    logging.info("="*80)
    
    return ensemble, results_df

# =============================================================================
# STEP 7: Prediction on New Data (Pure Polars)
# =============================================================================

def predict_new_data_polars(ensemble, new_df_polars, target_col='TOTAL_QUANTITY'):
    """
    Make predictions on new Polars DataFrame
    """
    
    logging.info("\nMaking predictions on new data...")
    
    # Prepare data
    X_new, _, _, _, _ = prepare_ensemble_data_polars(new_df_polars, target_col)
    
    # Predict
    predictions = ensemble.predict(X_new, method='weighted')
    
    # Add predictions to DataFrame
    result_df = new_df_polars.with_columns(
        pl.Series('prediction', predictions)
    )
    
    logging.info(f"✓ Predictions completed: {len(predictions)} rows")
    
    return result_df

# =============================================================================
# Run Pipeline
# =============================================================================

if __name__ == "__main__":
    # Example: Run complete pipeline
    # ensemble, results = main_polars_ensemble()
    
    pass
