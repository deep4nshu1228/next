# =============================================================================
# Train XGBoost with Customer and SKU as Categorical Features
# =============================================================================

import pandas as pd
import numpy as np
import xgboost as xgb
import logging
import os
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")

# -------------------------
# 0) Configuration
# -------------------------
BATCH_FILE = "features_batches/features_batch_0000.parquet"
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)

TARGET = "y"  # or "TOTAL_QUANTITY" or "y_log1p"
N_VALID_WEEKS = 8

# Columns to EXCLUDE from features (metadata only)
EXCLUDE_COLS = {
    'WEEK_START_DATE', 'WEEK_END_DATE',
    'FIRST_NONZERO_WEEK', 'ORDER_IDS', 'ORDER_DATES',
    'TOTAL_QUANTITY', 'y', 'y_log1p',  # Targets
    'TOTAL_DLP_VALUE', 'TOTAL_BASE_PRICE', 'BASE_PRICE'  # Raw values (already engineered)
}

# KEEP these as features:
# - CUSTOMER_EXTERNAL_CODE (learns customer buying patterns)
# - SKU_ERP_CODE (learns SKU demand patterns)

# -------------------------
# 1) Load Feature Batch
# -------------------------
logging.info(f"Loading feature batch: {BATCH_FILE}")
df = pd.read_parquet(BATCH_FILE)

logging.info(f"Loaded {len(df):,} rows, {df.shape[1]} columns")
logging.info(f"Unique customers: {df['CUSTOMER_EXTERNAL_CODE'].nunique():,}")
logging.info(f"Unique SKUs: {df['SKU_ERP_CODE'].nunique():,}")

# -------------------------
# 2) Train/Validation Split by Time
# -------------------------
max_week = df['WEEK_START_DATE'].max()
val_start = max_week - pd.to_timedelta(7 * (N_VALID_WEEKS - 1), unit="D")

logging.info(f"
Splitting data:")
logging.info(f"  Train: weeks < {val_start.date()}")
logging.info(f"  Valid: weeks >= {val_start.date()}")

train = df[df['WEEK_START_DATE'] < val_start].copy()
valid = df[df['WEEK_START_DATE'] >= val_start].copy()

# Save identifiers for later prediction mapping
valid_ids = valid[['CUSTOMER_EXTERNAL_CODE', 'SKU_ERP_CODE', 'WEEK_START_DATE']].copy()

logging.info(f"
Train: {len(train):,} rows")
logging.info(f"Valid: {len(valid):,} rows")

# -------------------------
# 3) Prepare Features and Target
# -------------------------
def prepare_features(df_input):
    """Extract features and target, encode categoricals"""
    
    # Get feature columns (including CUSTOMER and SKU)
    feature_cols = [c for c in df_input.columns if c not in EXCLUDE_COLS]
    
    X = df_input[feature_cols].copy()
    y = df_input[TARGET].values
    
    # Convert categorical columns to codes for XGBoost
    cat_cols = X.select_dtypes(include=['category', 'object']).columns.tolist()
    
    for col in cat_cols:
        X[col] = pd.Categorical(X[col]).codes
    
    logging.info(f"  Features: {len(feature_cols)} (including {len(cat_cols)} categorical)")
    logging.info(f"  Categorical features: {cat_cols}")
    
    return X.values, y, feature_cols, cat_cols

logging.info("
Preparing training data...")
X_train, y_train, feature_names, cat_features = prepare_features(train)

logging.info("Preparing validation data...")
X_valid, y_valid, _, _ = prepare_features(valid)

# -------------------------
# 4) Create XGBoost DMatrix
# -------------------------
logging.info("
Creating DMatrix...")
dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_names, enable_categorical=True)
dvalid = xgb.DMatrix(X_valid, label=y_valid, feature_names=feature_names, enable_categorical=True)

# -------------------------
# 5) XGBoost Parameters
# -------------------------
params = {
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'max_depth': 8,  # Increased for high-cardinality categoricals
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'min_child_weight': 5,  # Higher for regularization with many categories
    'gamma': 0,
    'reg_alpha': 0.1,
    'reg_lambda': 1.0,
    'tree_method': 'hist',
    'max_bin': 256,
    'max_cat_to_onehot': 4,  # Use optimal split for high-cardinality
    'random_state': 42,
    'n_jobs': -1
}

# -------------------------
# 6) Train Model
# -------------------------
logging.info("
" + "="*60)
logging.info("Training XGBoost Model")
logging.info("="*60)

evals = [(dtrain, 'train'), (dvalid, 'valid')]

model = xgb.train(
    params,
    dtrain,
    num_boost_round=1000,
    evals=evals,
    early_stopping_rounds=50,
    verbose_eval=50
)

# -------------------------
# 7) Save Model
# -------------------------
model_path = os.path.join(MODEL_DIR, "xgboost_demand_forecast.json")
model.save_model(model_path)
logging.info(f"
✓ Model saved to {model_path}")

# -------------------------
# 8) Evaluate Model
# -------------------------
logging.info("
" + "="*60)
logging.info("Model Evaluation")
logging.info("="*60)

y_pred = model.predict(dvalid)

rmse = np.sqrt(mean_squared_error(y_valid, y_pred))
mae = mean_absolute_error(y_valid, y_pred)
r2 = r2_score(y_valid, y_pred)

mask = y_valid > 0
mape = np.mean(np.abs((y_valid[mask] - y_pred[mask]) / y_valid[mask])) * 100 if mask.sum() > 0 else np.nan

logging.info(f"
RMSE:  {rmse:.4f}")
logging.info(f"MAE:   {mae:.4f}")
logging.info(f"R²:    {r2:.4f}")
logging.info(f"MAPE:  {mape:.2f}%")

# -------------------------
# 9) Sample Predictions with Customer/SKU Context
# -------------------------
logging.info("
" + "="*60)
logging.info("Sample Predictions (with Customer & SKU)")
logging.info("="*60)

sample_size = min(20, len(valid))
sample_idx = np.random.choice(len(valid), sample_size, replace=False)

results = pd.DataFrame({
    'CUSTOMER': valid_ids.iloc[sample_idx]['CUSTOMER_EXTERNAL_CODE'].values,
    'SKU': valid_ids.iloc[sample_idx]['SKU_ERP_CODE'].values,
    'WEEK': valid_ids.iloc[sample_idx]['WEEK_START_DATE'].dt.date.values,
    'actual': y_valid[sample_idx],
    'predicted': y_pred[sample_idx],
    'error': y_valid[sample_idx] - y_pred[sample_idx],
    'abs_pct_error': np.abs((y_valid[sample_idx] - y_pred[sample_idx]) / (y_valid[sample_idx] + 1e-9)) * 100
})

print(results.to_string(index=False))
results.to_csv(os.path.join(MODEL_DIR, "sample_predictions.csv"), index=False)

# -------------------------
# 10) Feature Importance (Check Customer/SKU Importance)
# -------------------------
importance = model.get_score(importance_type='gain')
importance_df = pd.DataFrame({
    'feature': list(importance.keys()),
    'importance': list(importance.values())
}).sort_values('importance', ascending=False)

logging.info("
Top 20 Important Features:")
print(importance_df.head(20).to_string(index=False))

# Check if CUSTOMER and SKU are in top features
customer_rank = importance_df[importance_df['feature'].str.contains('CUSTOMER', case=False, na=False)]
sku_rank = importance_df[importance_df['feature'].str.contains('SKU', case=False, na=False)]

if not customer_rank.empty:
    logging.info(f"
CUSTOMER_EXTERNAL_CODE rank: {customer_rank.index[0]+1} / {len(importance_df)}")
if not sku_rank.empty:
    logging.info(f"SKU_ERP_CODE rank: {sku_rank.index[0]+1} / {len(importance_df)}")

importance_df.to_csv(os.path.join(MODEL_DIR, "feature_importance.csv"), index=False)

logging.info("
" + "="*60)
logging.info("✓ Training Complete!")
logging.info("Model learns customer-SKU specific patterns for forecasting")
logging.info("="*60)
