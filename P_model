# =============================================================================
# Train XGBoost on Single Feature Batch with Train/Valid Split
# =============================================================================

import pandas as pd
import numpy as np
import xgboost as xgb
import logging
import os
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")

# -------------------------
# 0) Configuration
# -------------------------
BATCH_FILE = "features_batches/features_batch_0000.parquet"  # Your first batch
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)

TARGET = "y"  # or "TOTAL_QUANTITY" or "y_log1p" depending on what you created
N_VALID_WEEKS = 8

# Columns to exclude from features
EXCLUDE_COLS = {
    'CUSTOMER_EXTERNAL_CODE', 'SKU_ERP_CODE', 
    'WEEK_START_DATE', 'WEEK_END_DATE',
    'FIRST_NONZERO_WEEK', 'ORDER_IDS', 'ORDER_DATES',
    'TOTAL_QUANTITY', 'y', 'y_log1p',
    'TOTAL_DLP_VALUE', 'TOTAL_BASE_PRICE', 'BASE_PRICE'
}

# -------------------------
# 1) Load Feature Batch
# -------------------------
logging.info(f"Loading feature batch: {BATCH_FILE}")
df = pd.read_parquet(BATCH_FILE)

logging.info(f"Loaded {len(df):,} rows, {df.shape[1]} columns")
logging.info(f"Date range: {df['WEEK_START_DATE'].min()} to {df['WEEK_START_DATE'].max()}")

# -------------------------
# 2) Train/Validation Split by Time
# -------------------------
max_week = df['WEEK_START_DATE'].max()
val_start = max_week - pd.to_timedelta(7 * (N_VALID_WEEKS - 1), unit="D")

logging.info(f"
Splitting data:")
logging.info(f"  Train: weeks < {val_start.date()}")
logging.info(f"  Valid: weeks >= {val_start.date()}")

train = df[df['WEEK_START_DATE'] < val_start].copy()
valid = df[df['WEEK_START_DATE'] >= val_start].copy()

logging.info(f"
Train: {len(train):,} rows ({len(train)/len(df)*100:.1f}%)")
logging.info(f"Valid: {len(valid):,} rows ({len(valid)/len(df)*100:.1f}%)")

# -------------------------
# 3) Prepare Features and Target
# -------------------------
def prepare_features(df_input):
    """Extract features and target"""
    
    # Get feature columns
    feature_cols = [c for c in df_input.columns if c not in EXCLUDE_COLS]
    
    X = df_input[feature_cols].copy()
    y = df_input[TARGET].values
    
    # Handle categorical columns (convert to codes for XGBoost)
    cat_cols = X.select_dtypes(include=['category']).columns
    for col in cat_cols:
        X[col] = X[col].cat.codes
    
    logging.info(f"  Features: {len(feature_cols)}")
    logging.info(f"  Categorical features converted: {len(cat_cols)}")
    
    return X.values, y, feature_cols

logging.info("
Preparing training data...")
X_train, y_train, feature_names = prepare_features(train)

logging.info("Preparing validation data...")
X_valid, y_valid, _ = prepare_features(valid)

# -------------------------
# 4) Create XGBoost DMatrix
# -------------------------
logging.info("
Creating DMatrix...")
dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_names)
dvalid = xgb.DMatrix(X_valid, label=y_valid, feature_names=feature_names)

# -------------------------
# 5) XGBoost Parameters
# -------------------------
params = {
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'max_depth': 6,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'min_child_weight': 3,
    'gamma': 0,
    'reg_alpha': 0.1,
    'reg_lambda': 1.0,
    'tree_method': 'hist',  # Memory-efficient
    'max_bin': 256,
    'random_state': 42,
    'n_jobs': -1
}

# -------------------------
# 6) Train Model
# -------------------------
logging.info("
" + "="*60)
logging.info("Training XGBoost Model")
logging.info("="*60)

evals = [(dtrain, 'train'), (dvalid, 'valid')]

model = xgb.train(
    params,
    dtrain,
    num_boost_round=1000,
    evals=evals,
    early_stopping_rounds=50,
    verbose_eval=50
)

# -------------------------
# 7) Save Model
# -------------------------
model_path = os.path.join(MODEL_DIR, "xgboost_batch0.json")
model.save_model(model_path)
logging.info(f"
✓ Model saved to {model_path}")

# -------------------------
# 8) Evaluate Model
# -------------------------
logging.info("
" + "="*60)
logging.info("Model Evaluation on Validation Set")
logging.info("="*60)

y_pred = model.predict(dvalid)

# Metrics
rmse = np.sqrt(mean_squared_error(y_valid, y_pred))
mae = mean_absolute_error(y_valid, y_pred)
r2 = r2_score(y_valid, y_pred)

# MAPE (handle zeros)
mask = y_valid > 0
mape = np.mean(np.abs((y_valid[mask] - y_pred[mask]) / y_valid[mask])) * 100 if mask.sum() > 0 else np.nan

logging.info(f"
RMSE:  {rmse:.4f}")
logging.info(f"MAE:   {mae:.4f}")
logging.info(f"R²:    {r2:.4f}")
logging.info(f"MAPE:  {mape:.2f}%")

# Save metrics
metrics_df = pd.DataFrame([{
    'rmse': rmse,
    'mae': mae,
    'r2': r2,
    'mape': mape,
    'train_rows': len(train),
    'valid_rows': len(valid)
}])
metrics_df.to_csv(os.path.join(MODEL_DIR, "metrics_batch0.csv"), index=False)

# -------------------------
# 9) Feature Importance
# -------------------------
importance = model.get_score(importance_type='gain')

importance_df = pd.DataFrame({
    'feature': list(importance.keys()),
    'importance': list(importance.values())
}).sort_values('importance', ascending=False)

importance_df.to_csv(os.path.join(MODEL_DIR, "feature_importance_batch0.csv"), index=False)

logging.info("
Top 20 Important Features:")
print(importance_df.head(20).to_string(index=False))

# -------------------------
# 10) Sample Predictions
# -------------------------
logging.info("
" + "="*60)
logging.info("Sample Predictions")
logging.info("="*60)

sample_size = min(20, len(valid))
sample_idx = np.random.choice(len(valid), sample_size, replace=False)

results = pd.DataFrame({
    'CUSTOMER': valid.iloc[sample_idx]['CUSTOMER_EXTERNAL_CODE'].values,
    'SKU': valid.iloc[sample_idx]['SKU_ERP_CODE'].values,
    'WEEK': valid.iloc[sample_idx]['WEEK_START_DATE'].values,
    'actual': y_valid[sample_idx],
    'predicted': y_pred[sample_idx],
    'error': y_valid[sample_idx] - y_pred[sample_idx]
})

print(results.to_string(index=False))

results.to_csv(os.path.join(MODEL_DIR, "sample_predictions_batch0.csv"), index=False)

logging.info("
" + "="*60)
logging.info("✓ Training Complete!")
logging.info("="*60)
logging.info(f"Model saved: {model_path}")
logging.info(f"Metrics saved: {MODEL_DIR}/metrics_batch0.csv")
logging.info(f"Feature importance: {MODEL_DIR}/feature_importance_batch0.csv")
