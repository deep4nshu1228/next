# =============================================================================
# COMPLETE HYPERPARAMETER TUNING PIPELINE
# =============================================================================

import optuna
import xgboost as xgb
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import logging

# Setup logging
optuna.logging.set_verbosity(optuna.logging.INFO)

# Prepare data (assumes you have X_train, y_train, X_valid, y_valid ready)
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)

def objective(trial):
    params = {
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'tree_method': 'hist',
        'random_state': 42,
        
        # Tunable parameters
        'max_depth': trial.suggest_int('max_depth', 3, 6),
        'min_child_weight': trial.suggest_int('min_child_weight', 5, 20),
        'gamma': trial.suggest_float('gamma', 0, 5),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
        'subsample': trial.suggest_float('subsample', 0.7, 0.95),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.95),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.7, 0.95),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 5.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.5, 10.0, log=True),
    }
    
    n_estimators = trial.suggest_int('n_estimators', 100, 800)
    
    model = xgb.train(
        params,
        dtrain,
        num_boost_round=n_estimators,
        evals=[(dvalid, 'valid')],
        early_stopping_rounds=50,
        verbose_eval=False
    )
    
    y_pred = model.predict(dvalid)
    rmse = np.sqrt(mean_squared_error(y_valid, y_pred))
    
    trial.set_user_attr('mae', mean_absolute_error(y_valid, y_pred))
    trial.set_user_attr('r2', r2_score(y_valid, y_pred))
    
    return rmse

# Run optimization
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100, show_progress_bar=True)

# Print results
print("\n" + "="*60)
print("OPTIMIZATION RESULTS")
print("="*60)
print(f"\nBest RMSE: {study.best_value:.4f}")
print(f"Best MAE: {study.best_trial.user_attrs['mae']:.4f}")
print(f"Best R²: {study.best_trial.user_attrs['r2']:.4f}")
print("\nBest Parameters:")
for k, v in study.best_params.items():
    print(f"  {k}: {v}")

# Save study
import joblib
joblib.dump(study, 'optuna_study.pkl')
print("\n✓ Study saved to optuna_study.pkl")
