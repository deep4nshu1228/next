import numpy as np
from xgboost import XGBClassifier
from sklearn.metrics import balanced_accuracy_score, log_loss, classification_report, confusion_matrix

xgb = XGBClassifier(
    objective="multi:softprob",
    num_class=len(np.unique(y_train)),
    tree_method="hist",
    eval_metric="mlogloss",
    random_state=42,
    n_estimators=3000,
    learning_rate=0.07,
    max_depth=5,
    subsample=0.9,
    colsample_bytree=0.9,
    reg_lambda=1.0,
    reg_alpha=0.0,
    early_stopping_rounds=75,
    use_label_encoder=False
)

xgb.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)  # early stopping [8][11]
proba = xgb.predict_proba(X_test)
preds = xgb.predict(X_test)

print("Balanced Acc:", balanced_accuracy_score(y_test, preds))
print("Log loss:", log_loss(y_test, proba))
print(confusion_matrix(y_test, preds))
print(classification_report(y_test, preds, digits=4))




################################################################################

from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

cv_bal_acc, cv_logloss = [], []
for tr, va in skf.split(X_train, y_train):
    clf = XGBClassifier(
        objective="multi:softprob",
        num_class=len(np.unique(y_train)),
        tree_method="hist",
        eval_metric="mlogloss",
        random_state=42,
        n_estimators=3000,
        learning_rate=0.07,
        max_depth=5,
        subsample=0.9,
        colsample_bytree=0.9,
        reg_lambda=1.0,
        reg_alpha=0.0,
        early_stopping_rounds=50,
        use_label_encoder=False
    )
    clf.fit(X_train[tr], y_train[tr], eval_set=[(X_train[va], y_train[va])], verbose=False)
    p = clf.predict_proba(X_train[va])
    yhat = np.argmax(p, axis=1)
    cv_bal_acc.append(balanced_accuracy_score(y_train[va], yhat))
    cv_logloss.append(log_loss(y_train[va], p))

print("CV balanced acc:", np.mean(cv_bal_acc), "+/-", np.std(cv_bal_acc))
print("CV logloss:", np.mean(cv_logloss), "+/-", np.std(cv_logloss))




##############################################################################################

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform
from sklearn.model_selection import StratifiedKFold

param_dist = {
    "max_depth": randint(3, 9),
    "min_child_weight": randint(1, 10),
    "subsample": uniform(0.6, 0.4),
    "colsample_bytree": uniform(0.6, 0.4),
    "reg_lambda": uniform(0.0, 2.0),
    "reg_alpha": uniform(0.0, 1.0),
    "learning_rate": uniform(0.03, 0.17),
}

class XGBWithES(XGBClassifier):
    def fit(self, X, y, **kwargs):
        return super().fit(X, y, eval_set=[(X_test, y_test)], verbose=False, **kwargs)

search = RandomizedSearchCV(
    estimator=XGBWithES(
        objective="multi:softprob",
        num_class=len(np.unique(y_train)),
        tree_method="hist",
        eval_metric="mlogloss",
        random_state=42,
        n_estimators=3000,
        early_stopping_rounds=50,
        use_label_encoder=False
    ),
    param_distributions=param_dist,
    n_iter=40,
    scoring="balanced_accuracy",
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    n_jobs=-1,
    verbose=1,
    refit=True
)

search.fit(X_train, y_train)
best_xgb = search.best_estimator_
print("Best params:", search.best_params_)



##############################################################################


final_xgb = best_xgb
final_xgb.set_params(n_estimators=4000, early_stopping_rounds=100)
final_xgb.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)

proba = final_xgb.predict_proba(X_test)
preds = final_xgb.predict(X_test)
print("Balanced Acc:", balanced_accuracy_score(y_test, preds))
print("Log loss:", log_loss(y_test, proba))
print(confusion_matrix(y_test, preds))
print(classification_report(y_test, preds, digits=4))

