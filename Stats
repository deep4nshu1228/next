import pandas as pd
import numpy as np
from IPython.display import display

# --- CONFIG: update file path ---
excel_path = "your_warranty.xlsx"

# Sheet names (as provided)
model_sheet  = "MODEL_Analysis"
family_sheet = "MODEL_FAMILY_Analysis"

# Group columns (as provided)
model_group_col  = "Model"
family_group_col = "Model_family"

# Metric columns (as provided)
num_records_col  = "Number of Records Where Warranty Came In"
vehicles_col     = "Total Vehicles in Warranty"
total_cost_col   = "Total Warranty Cost"
claim_rate_col   = "Claims Rate (Number of Warranty Claims / Total Vehicles in Warranty)"
cost_per_claim_col = "Cost per Claim (Cost / Number of Warranty Claims)."
combined_metric_col = "Combined Metric (Rate * Cost per Claim)"

# We will prioritize volatility on cost metrics, then show others
cost_metric_priority = [cost_per_claim_col, total_cost_col, combined_metric_col]

# --- HELPERS ---
def normalize_headers(df):
    # keep original headers but add a lower/underscore alias map for robust access
    mapping = {c: c for c in df.columns}
    return df, mapping

def coerce_numeric_cols(df, cols):
    df = df.copy()
    for c in cols:
        if c in df.columns:
            if df[c].dtype == object:
                df[c] = df[c].astype(str).str.replace(",", "").str.strip()
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df

def cv_percent(series: pd.Series):
    s = pd.to_numeric(series, errors="coerce").dropna()
    if len(s) == 0:
        return np.nan
    mu = s.mean()
    if mu == 0:
        return np.nan
    std = s.std(ddof=1) if len(s) > 1 else 0.0  # robust when only 1 value
    return float(std / mu * 100.0)

def cv_by_group(df, group_col, metric_cols):
    rows = []
    for g, gdf in df.groupby(group_col, dropna=True):
        row = {group_col: g}
        for m in metric_cols:
            if m in gdf.columns:
                row[f"{m} | CV%"] = cv_percent(gdf[m])
        rows.append(row)
    out = pd.DataFrame(rows)
    return out

def pick_primary_cv(df, priority_cols):
    for m in priority_cols:
        col = f"{m} | CV%"
        if col in df.columns:
            return col
    # fallback: any CV column
    for c in df.columns:
        if c.endswith(" | CV%"):
            return c
    return None

# --- LOAD ---
model_df  = pd.read_excel(excel_path, sheet_name=model_sheet)
family_df = pd.read_excel(excel_path, sheet_name=family_sheet)

# --- PREP: coerce metric columns to numeric ---
metric_cols = [
    num_records_col, vehicles_col, total_cost_col,
    claim_rate_col, cost_per_claim_col, combined_metric_col
]
model_df  = coerce_numeric_cols(model_df, metric_cols)
family_df = coerce_numeric_cols(family_df, metric_cols)

# --- ANALYZE: CV by model and family for all provided metrics ---
model_cv  = cv_by_group(model_df,  model_group_col,  metric_cols)
family_cv = cv_by_group(family_df, family_group_col, metric_cols)

# Rank by priority: Cost per Claim, then Total Warranty Cost, then Combined Metric
primary_model_cv  = pick_primary_cv(model_cv,  cost_metric_priority)
primary_family_cv = pick_primary_cv(family_cv, cost_metric_priority)

if primary_model_cv:
    model_cv = model_cv.sort_values(primary_model_cv, ascending=False, na_position="last").reset_index(drop=True)
if primary_family_cv:
    family_cv = family_cv.sort_values(primary_family_cv, ascending=False, na_position="last").reset_index(drop=True)

print("Volatility ranking for Models (CV %): higher = more volatile")
print(f"Primary ranking column: {primary_model_cv}")
display(model_cv.head(25))

print("\nVolatility ranking for Model Families (CV %): higher = more volatile")
print(f"Primary ranking column: {primary_family_cv}")
display(family_cv.head(25))

# Optional: flag high volatility (e.g., CV > 30%) on the chosen primary metric
def flag_high_volatility(df, primary_col, threshold=30.0):
    if not primary_col or primary_col not in df.columns:
        return pd.DataFrame(columns=["Group", "Metric", "CV%"])
    group_col = df.columns[0]
    flags = df[[group_col, primary_col]].copy()
    flags = flags[flags[primary_col] > threshold].rename(columns={group_col: "Group", primary_col: "CV%"})
    flags["Metric"] = primary_col.replace(" | CV%", "")
    return flags.sort_values("CV%", ascending=False)

print("\nHigh-volatility Models (CV% > 30 on primary metric):")
display(flag_high_volatility(model_cv, primary_model_cv, threshold=30.0))

print("\nHigh-volatility Model Families (CV% > 30 on primary metric):")
display(flag_high_volatility(family_cv, primary_family_cv, threshold=30.0))









############################# test

# 1) Confirm multiple rows per model/family
print(model_df.groupby(model_group_col).size().describe())  # should be >1 per group on average [web:152]
print(family_df.groupby(family_group_col).size().describe())  # same check [web:152]

# 2) Check variance exists before grouping
for col in [cost_per_claim_col, total_cost_col, combined_metric_col]:
    if col in model_df.columns:
        print(col, "unique values count:", model_df[col].nunique())  # should be >1 to have variability [web:119]
        
# 3) Spot non-numeric remnants
print(model_df.dtypes)  # verify metrics are float, not object [web:92]
print(model_df.head(3))  # manual inspection for stray symbols [web:92]

# 4) Validate coercion
bad = model_df[[cost_per_claim_col]].copy()
bad["_isna"] = bad[cost_per_claim_col].isna()
print("NaN rate in cost_per_claim:", bad["_isna"].mean())  # high indicates parsing issues [web:92]





########################### outlier
import pandas as pd
import numpy as np
from scipy import stats
from IPython.display import display
import matplotlib.pyplot as plt

# --- CONFIG ---
excel_path = "your_warranty.xlsx"

# Load data
model_df = pd.read_excel(excel_path, sheet_name="MODEL_Analysis")
family_df = pd.read_excel(excel_path, sheet_name="MODEL_FAMILY_Analysis")

# Cost metric columns (adjust names if needed)
cost_per_claim_col = "Cost per Claim (Cost / Number of Warranty Claims)."
total_cost_col = "Total Warranty Cost"
combined_metric_col = "Combined Metric (Rate * Cost per Claim)"
claim_rate_col = "Claims Rate (Number of Warranty Claims / Total Vehicles in Warranty)"

# --- HELPER FUNCTIONS ---
def clean_numeric(series):
    """Clean and convert to numeric, handling commas and currency symbols"""
    return pd.to_numeric(series.astype(str).str.replace(",", "").str.replace("‚Çπ", "").str.strip(), errors="coerce")

def detect_outliers_zscore(df, column, model_col, threshold=2):
    """Detect outliers using Z-score method. Threshold=2 means 2 std devs from mean"""
    clean_values = clean_numeric(df[column])
    z_scores = np.abs(stats.zscore(clean_values, nan_policy='omit'))
    
    outliers = df[z_scores > threshold].copy()
    outliers[f'{column}_zscore'] = z_scores[z_scores > threshold]
    
    return outliers[[model_col, column, f'{column}_zscore']].sort_values(f'{column}_zscore', ascending=False)

def detect_outliers_iqr(df, column, model_col):
    """Detect outliers using IQR method (more robust to extreme values)"""
    clean_values = clean_numeric(df[column])
    
    Q1 = clean_values.quantile(0.25)
    Q3 = clean_values.quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df[(clean_values < lower_bound) | (clean_values > upper_bound)].copy()
    outliers[f'{column}_distance_from_bounds'] = np.where(
        clean_values < lower_bound, 
        lower_bound - clean_values,
        clean_values - upper_bound
    )[outliers.index]
    
    return outliers[[model_col, column, f'{column}_distance_from_bounds']].sort_values(f'{column}_distance_from_bounds', ascending=False)

def summary_stats(df, column):
    """Get descriptive statistics for the cost column"""
    clean_values = clean_numeric(df[column])
    return {
        'Mean': clean_values.mean(),
        'Median': clean_values.median(), 
        'Std Dev': clean_values.std(),
        'Min': clean_values.min(),
        'Max': clean_values.max(),
        'CV%': (clean_values.std() / clean_values.mean()) * 100 if clean_values.mean() != 0 else np.nan
    }

# --- ANALYSIS FOR MODELS ---
print("="*80)
print("OUTLIER ANALYSIS FOR BIKE MODELS")
print("="*80)

# Summary statistics for cost metrics
print(f"\nüìä SUMMARY STATISTICS:")
for col in [cost_per_claim_col, total_cost_col, combined_metric_col]:
    if col in model_df.columns:
        stats_dict = summary_stats(model_df, col)
        print(f"\n{col}:")
        for k, v in stats_dict.items():
            print(f"  {k}: {v:,.2f}" if pd.notna(v) else f"  {k}: {v}")

# Z-Score Outliers (2 standard deviations)
print(f"\nüö® HIGH COST OUTLIERS - Z-Score Method (>2œÉ):")
for col in [cost_per_claim_col, total_cost_col, combined_metric_col]:
    if col in model_df.columns:
        outliers_z = detect_outliers_zscore(model_df, col, "Model", threshold=2)
        if not outliers_z.empty:
            print(f"\n--- {col} ---")
            display(outliers_z.head(10))
        else:
            print(f"\n--- {col} ---")
            print("No outliers detected with Z-score > 2")

# IQR Method Outliers (more robust)
print(f"\nüìà OUTLIERS - IQR Method (beyond 1.5√óIQR):")
for col in [cost_per_claim_col, total_cost_col, combined_metric_col]:
    if col in model_df.columns:
        outliers_iqr = detect_outliers_iqr(model_df, col, "Model")
        if not outliers_iqr.empty:
            print(f"\n--- {col} ---")
            display(outliers_iqr.head(10))
        else:
            print(f"\n--- {col} ---")
            print("No outliers detected with IQR method")

# --- ANALYSIS FOR MODEL FAMILIES ---
print("\n" + "="*80)
print("OUTLIER ANALYSIS FOR MODEL FAMILIES")
print("="*80)

# Summary statistics for families
print(f"\nüìä SUMMARY STATISTICS:")
for col in [cost_per_claim_col, total_cost_col, combined_metric_col]:
    if col in family_df.columns:
        stats_dict = summary_stats(family_df, col)
        print(f"\n{col}:")
        for k, v in stats_dict.items():
            print(f"  {k}: {v:,.2f}" if pd.notna(v) else f"  {k}: {v}")

# Family outliers
print(f"\nüö® FAMILY OUTLIERS - Z-Score Method:")
for col in [cost_per_claim_col, total_cost_col, combined_metric_col]:
    if col in family_df.columns:
        outliers_z = detect_outliers_zscore(family_df, col, "Model_family", threshold=2)
        if not outliers_z.empty:
            print(f"\n--- {col} ---")
            display(outliers_z)

# --- INTERPRETATION GUIDE ---
print(f"\n" + "="*80)
print("üìñ INTERPRETATION GUIDE")
print("="*80)
print("""
Z-Score Outliers:
‚Ä¢ Z-score > 2: Moderately unusual (outside 95% of normal distribution)
‚Ä¢ Z-score > 3: Highly unusual (outside 99.7% of normal distribution)

IQR Outliers:
‚Ä¢ Beyond Q1 - 1.5√óIQR or Q3 + 1.5√óIQR
‚Ä¢ More robust to extreme values than Z-score
‚Ä¢ Standard method used in box plots

Coefficient of Variation (CV%):
‚Ä¢ CV < 15%: Low variability across models
‚Ä¢ CV 15-30%: Moderate variability  
‚Ä¢ CV > 30%: High variability (many outliers expected)

Action Items:
1. Investigate models with Z-score > 3 (highest priority)
2. Review models with Z-score > 2 for cost drivers
3. Compare IQR and Z-score results for consistency
""")

# Optional: Create a simple ranking of most extreme models
print(f"\nüèÜ TOP 10 HIGHEST COST MODELS:")
if cost_per_claim_col in model_df.columns:
    model_df_clean = model_df.copy()
    model_df_clean[cost_per_claim_col] = clean_numeric(model_df_clean[cost_per_claim_col])
    top_cost_models = model_df_clean.nlargest(10, cost_per_claim_col)
    display(top_cost_models[["Model", cost_per_claim_col, combined_metric_col]])

print(f"\nüí∞ TOP 10 LOWEST COST MODELS:")
if cost_per_claim_col in model_df.columns:
    bottom_cost_models = model_df_clean.nsmallest(10, cost_per_claim_col)
    display(bottom_cost_models[["Model", cost_per_claim_col, combined_metric_col]])




