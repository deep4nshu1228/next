# ‚úÖ SIMPLE SNOWPARK + IMPLICIT ALS (PANDAS VERSION - NO STORED PROCEDURES)
# Configurable QUANTITY/VALUE | City-wise models | Save models | Top 20 SKUs

from snowflake.snowpark import Session
from snowflake.snowpark.functions import col
import pandas as pd
import numpy as np
import implicit
from scipy.sparse import coo_matrix
import joblib
import os

# ==================== CONFIGURATION ====================
RATING_TYPE = "QUANTITY"  # "QUANTITY" or "VALUE" or "BOTH"
LATENT_FACTORS = 20
ALPHA = 15  # Implicit confidence scaling
ITERATIONS = 15
REGULARIZATION = 0.01
MODEL_SAVE_PATH = "./city_models/"  # Local path to save models

# Create model directory
os.makedirs(MODEL_SAVE_PATH, exist_ok=True)

# ==================== 1. SNOWPARK SESSION ====================
connection_parameters = {
    "account": "<your_account>",
    "user": "<your_user>", 
    "password": "<your_password>",
    "warehouse": "<your_warehouse>",
    "database": "<your_database>",
    "schema": "<your_schema>"
}
session = Session.builder.configs(connection_parameters).create()
print("‚úÖ Snowpark session created")

# ==================== 2. LOAD DATA (SNOWPARK ‚Üí PANDAS) ====================
print("üì• Loading last 6 months orders...")
orders_df = session.table("your_orders_table")  # UPDATE TABLE NAME

recent_orders = orders_df.select(
    col("CUSTOMER_EXTERNAL_CODE").alias("customer_id"),
    col("SKU_ERP_CODE").alias("sku"),
    col("DEALER_CITY").alias("city"),
    col("WEEK_START_DATE"),
    col("TOTAL_QUANTITY"),
    col("TOTAL_DLP_VALUE")
).filter(
    (col("WEEK_START_DATE") >= (session.sql("SELECT CURRENT_DATE() - 180").collect()[0][0])) &
    col("customer_id").is_not_null() &
    col("sku").is_not_null()
)

# Convert to pandas (one-time load)
data_pd = recent_orders.to_pandas()
print(f"‚úÖ Data loaded: {len(data_pd)} interactions")

# ==================== 3. CONFIGURE RATING ====================
if RATING_TYPE == "QUANTITY":
    data_pd['confidence'] = data_pd['TOTAL_QUANTITY']
elif RATING_TYPE == "VALUE":
    data_pd['confidence'] = data_pd['TOTAL_DLP_VALUE']
else:  # BOTH
    data_pd['confidence'] = data_pd['TOTAL_QUANTITY'] * data_pd['TOTAL_DLP_VALUE'] / 1000.0

# Filter positive confidence
data_pd = data_pd[data_pd['confidence'] > 0].reset_index(drop=True)
print(f"‚úÖ Rating type: {RATING_TYPE} | {len(data_pd)} valid interactions")

# ==================== 4. TRAIN IMPLICIT ALS CITY-WISE ====================
print("\nüöÄ Training Implicit ALS per city...")

city_models = {}
city_mappings = {}

cities = data_pd['city'].value_counts()
cities = cities[cities >= 500].index.tolist()  # Min 500 interactions per city

for city in cities:
    print(f"\nüìç Training: {city}")
    city_data = data_pd[data_pd['city'] == city].copy()
    
    # Create ID mappings
    city_data['customer_idx'] = city_data['customer_id'].astype('category').cat.codes
    city_data['sku_idx'] = city_data['sku'].astype('category').cat.codes
    
    customer_map = dict(enumerate(city_data['customer_id'].astype('category').cat.categories))
    sku_map = dict(enumerate(city_data['sku'].astype('category').cat.categories))
    
    # Reverse mappings
    customer_to_idx = {v: k for k, v in customer_map.items()}
    sku_to_idx = {v: k for k, v in sku_map.items()}
    
    # Create sparse matrix (users √ó items)
    sparse_matrix = coo_matrix(
        (city_data['confidence'].values, 
         (city_data['customer_idx'].values, city_data['sku_idx'].values))
    ).tocsr()
    
    print(f"   Matrix: {sparse_matrix.shape[0]} users √ó {sparse_matrix.shape[1]} SKUs")
    
    # Train Implicit ALS
    model = implicit.als.AlternatingLeastSquares(
        factors=LATENT_FACTORS,
        alpha=ALPHA,
        iterations=ITERATIONS,
        regularization=REGULARIZATION,
        use_gpu=False,
        calculate_training_loss=True
    )
    
    model.fit(sparse_matrix)
    
    # Save model and mappings
    city_models[city] = model
    city_mappings[city] = {
        'customer_map': customer_map,
        'sku_map': sku_map,
        'customer_to_idx': customer_to_idx,
        'sku_to_idx': sku_to_idx,
        'sparse_matrix': sparse_matrix
    }
    
    # Save to disk
    model_file = os.path.join(MODEL_SAVE_PATH, f"{city.replace(' ', '_')}_model.pkl")
    mapping_file = os.path.join(MODEL_SAVE_PATH, f"{city.replace(' ', '_')}_mappings.pkl")
    
    joblib.dump(model, model_file)
    joblib.dump(city_mappings[city], mapping_file)
    
    print(f"   ‚úÖ Training loss: {model.solver.loss:.4f}")
    print(f"   ‚úÖ Saved: {model_file}")

print(f"\n‚úÖ Trained {len(city_models)} city models")

# ==================== 5. RECOMMENDATION FUNCTION ====================
def get_recommendations(customer_id, city, top_k=20, exclude_recent=True):
    """Get top K recommendations for customer"""
    
    if city not in city_models:
        print(f"‚ùå No model for city: {city}")
        return pd.DataFrame()
    
    model = city_models[city]
    mappings = city_mappings[city]
    
    # Check if customer exists
    if customer_id not in mappings['customer_to_idx']:
        print(f"‚ùå Customer {customer_id} not found in {city}")
        return pd.DataFrame()
    
    customer_idx = mappings['customer_to_idx'][customer_id]
    
    # Get recommendations (returns indices + scores)
    recs = model.recommend(
        customer_idx, 
        mappings['sparse_matrix'][customer_idx],
        N=top_k * 2,  # Get extra for filtering
        filter_already_liked_items=exclude_recent
    )
    
    # Map back to SKU codes
    recommendations = []
    for sku_idx, score in zip(recs[0], recs[1]):
        sku_code = mappings['sku_map'][sku_idx]
        recommendations.append({
            'customer_id': customer_id,
            'city': city,
            'sku': sku_code,
            'preference_score': float(score)
        })
    
    return pd.DataFrame(recommendations[:top_k])

# ==================== 6. TEST RECOMMENDATIONS ====================
TEST_CUSTOMER = "CUST12345"  # UPDATE WITH REAL CUSTOMER_EXTERNAL_CODE
TEST_CITY = "Delhi"          # UPDATE WITH REAL DEALER_CITY

print(f"\nüéØ TOP 20 RECOMMENDATIONS:")
print(f"Customer: {TEST_CUSTOMER} | City: {TEST_CITY} | Rating: {RATING_TYPE}")

try:
    recs_df = get_recommendations(TEST_CUSTOMER, TEST_CITY, top_k=20)
    print(recs_df)
    
    # Save back to Snowflake
    if not recs_df.empty:
        snow_recs = session.create_dataframe(recs_df)
        snow_recs.write.mode("overwrite").save_as_table(f"RECS_{TEST_CUSTOMER.replace(' ', '_').replace('/', '_')}")
        print(f"\n‚úÖ Saved to Snowflake: RECS_{TEST_CUSTOMER}")
except Exception as e:
    print(f"‚ùå Error: {e}")

# ==================== 7. BATCH RECOMMENDATIONS (ALL CUSTOMERS IN CITY) ====================
def generate_city_batch_recommendations(city, top_k=20):
    """Generate recommendations for ALL customers in a city"""
    
    if city not in city_models:
        print(f"‚ùå No model for {city}")
        return pd.DataFrame()
    
    model = city_models[city]
    mappings = city_mappings[city]
    
    all_recs = []
    customers = list(mappings['customer_map'].values())
    
    print(f"üì¶ Generating batch for {len(customers)} customers in {city}...")
    
    for i, customer_id in enumerate(customers):
        if i % 100 == 0:
            print(f"   Progress: {i}/{len(customers)}")
        
        try:
            recs = get_recommendations(customer_id, city, top_k=top_k)
            all_recs.append(recs)
        except:
            continue
    
    return pd.concat(all_recs, ignore_index=True) if all_recs else pd.DataFrame()

# Generate batch for test city
print(f"\nüì¶ Generating batch recommendations for {TEST_CITY}...")
city_batch = generate_city_batch_recommendations(TEST_CITY, top_k=20)

if not city_batch.empty:
    print(f"‚úÖ Generated {len(city_batch)} recommendations for {city_batch['customer_id'].nunique()} customers")
    
    # Save to Snowflake
    snow_batch = session.create_dataframe(city_batch)
    snow_batch.write.mode("overwrite").save_as_table(f"BATCH_RECS_{TEST_CITY.replace(' ', '_')}")
    print(f"‚úÖ Saved to Snowflake: BATCH_RECS_{TEST_CITY}")

# ==================== 8. LOAD MODEL FROM DISK (FOR PRODUCTION) ====================
def load_city_model(city):
    """Load saved model from disk"""
    model_file = os.path.join(MODEL_SAVE_PATH, f"{city.replace(' ', '_')}_model.pkl")
    mapping_file = os.path.join(MODEL_SAVE_PATH, f"{city.replace(' ', '_')}_mappings.pkl")
    
    if not os.path.exists(model_file):
        print(f"‚ùå Model not found: {model_file}")
        return None, None
    
    model = joblib.load(model_file)
    mappings = joblib.load(mapping_file)
    return model, mappings

# Example: Load model later
# loaded_model, loaded_mappings = load_city_model("Delhi")

# ==================== 9. SUMMARY ====================
print("\n" + "="*60)
print("üéâ IMPLICIT ALS TRAINING COMPLETE!")
print("="*60)
print(f"‚úÖ Rating Type: {RATING_TYPE}")
print(f"‚úÖ Latent Factors: {LATENT_FACTORS}")
print(f"‚úÖ Cities Trained: {len(city_models)}")
print(f"‚úÖ Models Saved: {MODEL_SAVE_PATH}")
print(f"‚úÖ Total Data: {len(data_pd):,} interactions")
print("="*60)

session.close()
print("‚úÖ Session closed - Ready for Talverge production!")
