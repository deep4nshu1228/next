# ‚úÖ FIXED: ALL ALIASES CAPITAL | SIMPLE SNOWPARK + IMPLICIT ALS (PANDAS VERSION)
# Configurable QUANTITY/VALUE | City-wise models | Save models | Top 20 SKUs

from snowflake.snowpark import Session
from snowflake.snowpark.functions import col, upper
import pandas as pd
import numpy as np
import implicit
from scipy.sparse import coo_matrix
import joblib
import os

# ==================== CONFIGURATION ====================
RATING_TYPE = "QUANTITY"  # "QUANTITY" or "VALUE" or "BOTH"
LATENT_FACTORS = 20
ALPHA = 15  # Implicit confidence scaling
ITERATIONS = 15
REGULARIZATION = 0.01
MODEL_SAVE_PATH = "./city_models/"  # Local path to save models

# Create model directory
os.makedirs(MODEL_SAVE_PATH, exist_ok=True)

# ==================== 1. SNOWPARK SESSION ====================
connection_parameters = {
    "account": "<your_account>",
    "user": "<your_user>", 
    "password": "<your_password>",
    "warehouse": "<your_warehouse>",
    "database": "<your_database>",
    "schema": "<your_schema>"
}
session = Session.builder.configs(connection_parameters).create()
print("‚úÖ Snowpark session created")

# ==================== 2. LOAD DATA (SNOWPARK ‚Üí PANDAS) ====================
print("üì• Loading last 6 months orders...")
orders_df = session.table("your_orders_table")  # UPDATE TABLE NAME

recent_orders = orders_df.select(
    upper(col("CUSTOMER_EXTERNAL_CODE")).alias("CUSTOMER_ID"),
    upper(col("SKU_ERP_CODE")).alias("SKU"),
    upper(col("DEALER_CITY")).alias("CITY"),
    col("WEEK_START_DATE"),
    col("TOTAL_QUANTITY"),
    col("TOTAL_DLP_VALUE")
).filter(
    (col("WEEK_START_DATE") >= (session.sql("SELECT CURRENT_DATE() - 180").collect()[0][0])) &
    col("CUSTOMER_ID").is_not_null() &
    col("SKU").is_not_null()
)

# Convert to pandas (one-time load)
data_pd = recent_orders.to_pandas()
print(f"‚úÖ Data loaded: {len(data_pd)} interactions")

# ==================== 3. CONFIGURE RATING ====================
if RATING_TYPE == "QUANTITY":
    data_pd['CONFIDENCE'] = data_pd['TOTAL_QUANTITY']
elif RATING_TYPE == "VALUE":
    data_pd['CONFIDENCE'] = data_pd['TOTAL_DLP_VALUE']
else:  # BOTH
    data_pd['CONFIDENCE'] = data_pd['TOTAL_QUANTITY'] * data_pd['TOTAL_DLP_VALUE'] / 1000.0

# Filter positive confidence
data_pd = data_pd[data_pd['CONFIDENCE'] > 0].reset_index(drop=True)

# ‚úÖ ENSURE CITY IS UPPERCASE IN PANDAS TOO
data_pd['CITY'] = data_pd['CITY'].str.upper()

print(f"‚úÖ Rating type: {RATING_TYPE} | {len(data_pd)} valid interactions")
print(f"‚úÖ Cities found: {data_pd['CITY'].nunique()}")
print(f"‚úÖ Sample cities: {data_pd['CITY'].value_counts().head().to_dict()}")

# ==================== 4. TRAIN IMPLICIT ALS CITY-WISE ====================
print("\nüöÄ Training Implicit ALS per city...")

city_models = {}
city_mappings = {}

cities = data_pd['CITY'].value_counts()
cities = cities[cities >= 500].index.tolist()  # Min 500 interactions per city

for city in cities:
    print(f"\nüìç Training: {city}")
    city_data = data_pd[data_pd['CITY'] == city].copy()
    
    # Create ID mappings
    city_data['CUSTOMER_IDX'] = city_data['CUSTOMER_ID'].astype('category').cat.codes
    city_data['SKU_IDX'] = city_data['SKU'].astype('category').cat.codes
    
    customer_map = dict(enumerate(city_data['CUSTOMER_ID'].astype('category').cat.categories))
    sku_map = dict(enumerate(city_data['SKU'].astype('category').cat.categories))
    
    # Reverse mappings
    customer_to_idx = {v: k for k, v in customer_map.items()}
    sku_to_idx = {v: k for k, v in sku_map.items()}
    
    # Create sparse matrix (users √ó items)
    sparse_matrix = coo_matrix(
        (city_data['CONFIDENCE'].values, 
         (city_data['CUSTOMER_IDX'].values, city_data['SKU_IDX'].values))
    ).tocsr()
    
    print(f"   Matrix: {sparse_matrix.shape[0]} users √ó {sparse_matrix.shape[1]} SKUs")
    
    # Train Implicit ALS
    model = implicit.als.AlternatingLeastSquares(
        factors=LATENT_FACTORS,
        alpha=ALPHA,
        iterations=ITERATIONS,
        regularization=REGULARIZATION,
        use_gpu=False,
        calculate_training_loss=True
    )
    
    model.fit(sparse_matrix)
    
    # Save model and mappings
    city_models[city] = model
    city_mappings[city] = {
        'CUSTOMER_MAP': customer_map,
        'SKU_MAP': sku_map,
        'CUSTOMER_TO_IDX': customer_to_idx,
        'SKU_TO_IDX': sku_to_idx,
        'SPARSE_MATRIX': sparse_matrix
    }
    
    # Save to disk (safe filename)
    safe_city_name = city.replace(' ', '_').replace('/', '_')
    model_file = os.path.join(MODEL_SAVE_PATH, f"{safe_city_name}_model.pkl")
    mapping_file = os.path.join(MODEL_SAVE_PATH, f"{safe_city_name}_mappings.pkl")
    
    joblib.dump(model, model_file)
    joblib.dump(city_mappings[city], mapping_file)
    
    print(f"   ‚úÖ Training loss: {model.solver.loss:.4f}")
    print(f"   ‚úÖ Saved: {model_file}")

print(f"\n‚úÖ Trained {len(city_models)} city models")

# ==================== 5. RECOMMENDATION FUNCTION ====================
def get_recommendations(CUSTOMER_ID, CITY, top_k=20, exclude_recent=True):
    """Get top K recommendations for customer"""
    
    # ‚úÖ NORMALIZE CITY INPUT TO UPPERCASE
    CITY = CITY.upper()
    
    if CITY not in city_models:
        available_cities = list(city_models.keys())
        print(f"‚ùå No model for city: '{CITY}'")
        print(f"   Available cities: {available_cities[:5]}")
        return pd.DataFrame()
    
    model = city_models[CITY]
    mappings = city_mappings[CITY]
    
    # Check if customer exists
    if CUSTOMER_ID not in mappings['CUSTOMER_TO_IDX']:
        print(f"‚ùå Customer {CUSTOMER_ID} not found in {CITY}")
        print(f"   Sample customers: {list(mappings['CUSTOMER_TO_IDX'].keys())[:5]}")
        return pd.DataFrame()
    
    customer_idx = mappings['CUSTOMER_TO_IDX'][CUSTOMER_ID]
    
    # Get recommendations (returns indices + scores)
    recs = model.recommend(
        customer_idx, 
        mappings['SPARSE_MATRIX'][customer_idx],
        N=top_k * 2,  # Get extra for filtering
        filter_already_liked_items=exclude_recent
    )
    
    # Map back to SKU codes
    recommendations = []
    for sku_idx, score in zip(recs[0], recs[1]):
        sku_code = mappings['SKU_MAP'][sku_idx]
        recommendations.append({
            'CUSTOMER_ID': CUSTOMER_ID,
            'CITY': CITY,
            'SKU': sku_code,
            'PREFERENCE_SCORE': float(score)
        })
    
    return pd.DataFrame(recommendations[:top_k])

# ==================== 6. TEST RECOMMENDATIONS ====================
TEST_CUSTOMER_ID = "CUST12345"  # UPDATE WITH REAL CUSTOMER_EXTERNAL_CODE
TEST_CITY = "DELHI"             # UPDATE WITH REAL DEALER_CITY (UPPERCASE)

print(f"\nüéØ TOP 20 RECOMMENDATIONS:")
print(f"CUSTOMER_ID: {TEST_CUSTOMER_ID} | CITY: {TEST_CITY} | Rating: {RATING_TYPE}")

# ‚úÖ DEBUG: Show available cities first
print(f"\nüìç Available cities in models: {list(city_models.keys())}")

try:
    recs_df = get_recommendations(TEST_CUSTOMER_ID, TEST_CITY, top_k=20)
    
    if not recs_df.empty:
        print(recs_df)
        
        # Save back to Snowflake
        snow_recs = session.create_dataframe(recs_df)
        safe_customer_name = TEST_CUSTOMER_ID.replace(' ', '_').replace('/', '_')
        snow_recs.write.mode("overwrite").save_as_table(f"RECS_{safe_customer_name}")
        print(f"\n‚úÖ Saved to Snowflake: RECS_{safe_customer_name}")
    else:
        print("‚ö†Ô∏è No recommendations generated")
        
except Exception as e:
    print(f"‚ùå Error: {e}")
    import traceback
    traceback.print_exc()

# ==================== 7. BATCH RECOMMENDATIONS (ALL CUSTOMERS IN CITY) ====================
def generate_city_batch_recommendations(CITY, top_k=20):
    """Generate recommendations for ALL customers in a city"""
    
    # ‚úÖ NORMALIZE CITY
    CITY = CITY.upper()
    
    if CITY not in city_models:
        print(f"‚ùå No model for {CITY}")
        return pd.DataFrame()
    
    model = city_models[CITY]
    mappings = city_mappings[CITY]
    
    all_recs = []
    customers = list(mappings['CUSTOMER_MAP'].values())
    
    print(f"üì¶ Generating batch for {len(customers)} customers in {CITY}...")
    
    for i, customer_id in enumerate(customers):
        if i % 100 == 0:
            print(f"   Progress: {i}/{len(customers)}")
        
        try:
            recs = get_recommendations(customer_id, CITY, top_k=top_k)
            all_recs.append(recs)
        except:
            continue
    
    return pd.concat(all_recs, ignore_index=True) if all_recs else pd.DataFrame()

# Generate batch for test city
print(f"\nüì¶ Generating batch recommendations for {TEST_CITY}...")
city_batch = generate_city_batch_recommendations(TEST_CITY, top_k=20)

if not city_batch.empty:
    print(f"‚úÖ Generated {len(city_batch)} recommendations for {city_batch['CUSTOMER_ID'].nunique()} customers")
    
    # Save to Snowflake
    snow_batch = session.create_dataframe(city_batch)
    safe_city_name = TEST_CITY.replace(' ', '_').replace('/', '_')
    snow_batch.write.mode("overwrite").save_as_table(f"BATCH_RECS_{safe_city_name}")
    print(f"‚úÖ Saved to Snowflake: BATCH_RECS_{safe_city_name}")

# ==================== 8. LOAD MODEL FROM DISK (FOR PRODUCTION) ====================
def load_city_model(CITY):
    """Load saved model from disk"""
    # ‚úÖ NORMALIZE CITY
    safe_city_name = CITY.upper().replace(' ', '_').replace('/', '_')
    model_file = os.path.join(MODEL_SAVE_PATH, f"{safe_city_name}_model.pkl")
    mapping_file = os.path.join(MODEL_SAVE_PATH, f"{safe_city_name}_mappings.pkl")
    
    if not os.path.exists(model_file):
        print(f"‚ùå Model not found: {model_file}")
        return None, None
    
    model = joblib.dump(model_file)
    mappings = joblib.load(mapping_file)
    return model, mappings

# ==================== 9. SUMMARY ====================
print("\n" + "="*60)
print("üéâ IMPLICIT ALS TRAINING COMPLETE!")
print("="*60)
print(f"‚úÖ Rating Type: {RATING_TYPE}")
print(f"‚úÖ Latent Factors: {LATENT_FACTORS}")
print(f"‚úÖ Cities Trained: {len(city_models)}")
print(f"‚úÖ City Names: {list(city_models.keys())[:10]}")
print(f"‚úÖ Models Saved: {MODEL_SAVE_PATH}")
print(f"‚úÖ Total Data: {len(data_pd):,} interactions")
print("="*60)

session.close()
print("‚úÖ Session closed - Ready for Talverge production!")
