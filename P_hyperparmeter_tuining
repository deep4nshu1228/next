from xgboost import XGBRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import make_scorer, mean_squared_error
import numpy as np

# X_train, y_train, X_valid, y_valid already prepared (numpy or array-like)

# 1) Base model
xgb = XGBRegressor(
    objective="reg:squarederror",
    tree_method="hist",
    random_state=42,
    n_estimators=1000  # will be tuned via early_stopping
)

# 2) Hyperparameter search space (kept small and sane)
param_dist = {
    "max_depth":        [3, 4, 5, 6, 8],
    "learning_rate":    [0.01, 0.03, 0.05, 0.1],
    "min_child_weight": [1, 3, 5, 10],
    "subsample":        [0.6, 0.8, 1.0],
    "colsample_bytree": [0.6, 0.8, 1.0],
    "gamma":            [0, 0.1, 0.5, 1.0],
    "reg_alpha":        [0, 0.1, 1, 5],
    "reg_lambda":       [0.5, 1, 5, 10],
}

# 3) RMSE scorer (RandomizedSearchCV maximizes the score, so return negative RMSE)
def neg_rmse(y_true, y_pred):
    return -np.sqrt(mean_squared_error(y_true, y_pred))

rmse_scorer = make_scorer(neg_rmse, greater_is_better=True)

# 4) Set up RandomizedSearchCV
search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=param_dist,
    n_iter=40,              # number of random combos; increase if time allows
    scoring=rmse_scorer,
    cv=3,                   # 3-fold CV
    verbose=1,
    n_jobs=-1,
    random_state=42,
)

# 5) Run search
search.fit(X_train, y_train)

print("Best params:", search.best_params_)
print("Best CV RMSE:", -search.best_score_)

best_model = search.best_estimator_

# 6) Optional: refit with early stopping on your own validation set
best_model.set_params(n_estimators=5000)
best_model.fit(
    X_train, y_train,
    eval_set=[(X_valid, y_valid)],
    eval_metric="rmse",
    verbose=False,
    early_stopping_rounds=100,
)

print("Best iteration:", best_model.best_iteration)
