from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import make_scorer, f1_score
import numpy as np

# Define the model
xgb_model = XGBClassifier(
    objective='multi:softprob',  # output probabilities for multiclass
    num_class=len(le.classes_),
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42,
    n_jobs=-1
)

# Define parameter space to explore
param_dist = {
    'n_estimators': np.arange(100, 1000, 100),
    'max_depth': np.arange(3, 12, 1),
    'learning_rate': np.linspace(0.01, 0.3, 10),
    'subsample': np.linspace(0.6, 1.0, 5),
    'colsample_bytree': np.linspace(0.6, 1.0, 5),
    'gamma': np.linspace(0, 5, 6),
    'min_child_weight': np.arange(1, 10, 1)
}

# Use weighted F1-score as scoring (good for imbalanced data)
f1_weighted = make_scorer(f1_score, average='weighted')

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_dist,
    n_iter=50,  # number of parameter settings sampled; increase for more exhaustive search
    scoring=f1_weighted,
    cv=3,
    verbose=2,
    n_jobs=-1,
    random_state=42,
    refit=True
)

# Fit on training data
random_search.fit(X_train, y_train_enc, sample_weight=sample_weights)  # include sample_weight if used

# Best hyperparameters and score
print("Best parameters found:", random_search.best_params_)
print("Best weighted F1 score:", random_search.best_score_)

# Use best estimator for predictions
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test_proc)






################################# Optimized Params ###################################
from xgboost import XGBClassifier

# Example optimized parameters (replace with your actual best params)
optimized_params = {
  'n_estimators': 400,
  'max_depth': 6,
  'learning_rate': 0.05,
  'subsample': 0.8,
  'colsample_bytree': 0.8,
  'gamma': 1.0,
  'min_child_weight': 3,
  'objective': 'multi:softprob',
  'num_class': len(le.classes_),
  'use_label_encoder': False,
  'eval_metric': 'mlogloss',
  'random_state': 42
}

# Create model with optimized params
xgb_optimized = XGBClassifier(**optimized_params)

# Fit model on balanced training data, with sample weights if used
xgb_optimized.fit(X_train_bal, y_train_bal)  # add sample_weight=sample_weights if applicable

# Predict on preprocessed test set
y_pred = xgb_optimized.predict(X_test_proc)

# Decode labels back to original (if needed)
y_test_labels = le.inverse_transform(y_test)
y_pred_labels = le.inverse_transform(y_pred)

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score
import matplotlib.pyplot as plt

print("Classification report (Optimized XGBoost):")
print(classification_report(y_test_labels, y_pred_labels))

acc = accuracy_score(y_test_labels, y_pred_labels)
f1w = f1_score(y_test_labels, y_pred_labels, average='weighted')
print(f"Accuracy: {acc:.4f} | Weighted F1: {f1w:.4f}")

cm = confusion_matrix(y_test_labels, y_pred_labels, labels=le.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
disp.plot(cmap='Blues', xticks_rotation=45)
plt.title('Confusion Matrix - XGBClassifier Optimized')
plt.show()

