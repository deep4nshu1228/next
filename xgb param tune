from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import make_scorer, f1_score
import numpy as np

# Define the model
xgb_model = XGBClassifier(
    objective='multi:softprob',  # output probabilities for multiclass
    num_class=len(le.classes_),
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42,
    n_jobs=-1
)

# Define parameter space to explore
param_dist = {
    'n_estimators': np.arange(100, 1000, 100),
    'max_depth': np.arange(3, 12, 1),
    'learning_rate': np.linspace(0.01, 0.3, 10),
    'subsample': np.linspace(0.6, 1.0, 5),
    'colsample_bytree': np.linspace(0.6, 1.0, 5),
    'gamma': np.linspace(0, 5, 6),
    'min_child_weight': np.arange(1, 10, 1)
}

# Use weighted F1-score as scoring (good for imbalanced data)
f1_weighted = make_scorer(f1_score, average='weighted')

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_dist,
    n_iter=50,  # number of parameter settings sampled; increase for more exhaustive search
    scoring=f1_weighted,
    cv=3,
    verbose=2,
    n_jobs=-1,
    random_state=42,
    refit=True
)

# Fit on training data
random_search.fit(X_train, y_train_enc, sample_weight=sample_weights)  # include sample_weight if used

# Best hyperparameters and score
print("Best parameters found:", random_search.best_params_)
print("Best weighted F1 score:", random_search.best_score_)

# Use best estimator for predictions
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test_proc)
