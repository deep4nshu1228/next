# =============================================================================
# Memory-Efficient Feature Engineering for 16M Rows on 8GB RAM
# Uses customer-based chunking to process data in batches
# =============================================================================

import pandas as pd
import numpy as np
import logging
import os
import gc

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")

# -------------------------
# 0) Initial Setup and Memory Optimization
# -------------------------
# Define dtypes upfront to reduce memory on load
DTYPES = {
    'CUSTOMER_EXTERNAL_CODE': 'category',
    'SKU_ERP_CODE': 'category',
    'BASE_PRICE': 'float32',
    'TOTAL_QUANTITY': 'float32',
    'TOTAL_DLP_VALUE': 'float32'
}

DATE_COLS = ['WEEK_START_DATE', 'WEEK_END_DATE']

# -------------------------
# 1) Load Customer List Only (Minimal Memory)
# -------------------------
logging.info("Loading customer list...")
customers = pd.read_parquet(
    'your_data.parquet',
    columns=['CUSTOMER_EXTERNAL_CODE']
)['CUSTOMER_EXTERNAL_CODE'].unique()

logging.info(f"Total customers: {len(customers):,}")

# -------------------------
# 2) Define Batch Size and Chunking Strategy
# -------------------------
BATCH_SIZE = 2000  # Process 2000 customers at a time (adjust based on memory)
num_batches = int(np.ceil(len(customers) / BATCH_SIZE))

logging.info(f"Processing in {num_batches} batches of ~{BATCH_SIZE} customers each")

# -------------------------
# 3) Feature Engineering Function (Memory-Conscious)
# -------------------------
def create_features_chunk(df_chunk):
    """
    Create features for a subset of customers
    Returns only essential features to minimize memory
    """
    GROUP_KEYS = ["CUSTOMER_EXTERNAL_CODE", "SKU_ERP_CODE"]
    TARGET = "TOTAL_QUANTITY"
    
    # Sort for time-series operations
    df_chunk = df_chunk.sort_values(GROUP_KEYS + ["WEEK_START_DATE"])
    
    # Derived fields
    df_chunk["UNIT_DLP_PRICE"] = np.where(
        df_chunk[TARGET] > 0,
        df_chunk["TOTAL_DLP_VALUE"] / df_chunk[TARGET],
        np.nan
    ).astype('float32')
    
    # Temporal features (minimal set)
    iso = df_chunk["WEEK_START_DATE"].dt.isocalendar()
    df_chunk["YEAR"] = iso.year.astype("int16")
    df_chunk["WEEK"] = iso.week.astype("int8")
    df_chunk["MONTH"] = df_chunk["WEEK_START_DATE"].dt.month.astype("int8")
    df_chunk["WEEK_SIN"] = np.sin(2*np.pi*df_chunk["WEEK"]/52.0).astype('float32')
    df_chunk["WEEK_COS"] = np.cos(2*np.pi*df_chunk["WEEK"]/52.0).astype('float32')
    
    # Core lags (limited set for memory)
    def add_lag_features(g):
        for L in [1, 2, 4, 8]:  # Reduced lag set
            g[f"qty_lag{L}"] = g[TARGET].shift(L).astype('float32')
        return g
    
    df_chunk = df_chunk.groupby(GROUP_KEYS, group_keys=False).apply(add_lag_features)
    
    # Rolling features (essential only)
    def add_rolling_features(g):
        s = g[TARGET].shift(1)
        for w in [4, 8]:  # Only 4 and 8 week windows
            g[f"qty_rollmean_{w}"] = s.rolling(w, min_periods=1).mean().astype('float32')
            g[f"qty_rollstd_{w}"] = s.rolling(w, min_periods=2).std().astype('float32')
        return g
    
    df_chunk = df_chunk.groupby(GROUP_KEYS, group_keys=False).apply(add_rolling_features)
    
    # Exponential smoothing (minimal)
    def add_ewm_features(g):
        s = g[TARGET].shift(1)
        g["qty_ewm_mean_h4"] = s.ewm(halflife=4, min_periods=1, adjust=False).mean().astype('float32')
        return g
    
    df_chunk = df_chunk.groupby(GROUP_KEYS, group_keys=False).apply(add_ewm_features)
    
    # Intermittency
    def add_intermittency(g):
        nz = (g[TARGET].shift(1).fillna(0) > 0).astype(int)
        g["nonzero_ratio_8"] = nz.rolling(8, min_periods=1).mean().astype('float32')
        return g
    
    df_chunk = df_chunk.groupby(GROUP_KEYS, group_keys=False).apply(add_intermittency)
    
    # Fill NaNs
    feature_cols = [c for c in df_chunk.columns if c.startswith(('qty_', 'nonzero_'))]
    df_chunk[feature_cols] = df_chunk.groupby(GROUP_KEYS)[feature_cols].apply(
        lambda g: g.fillna(method='ffill').fillna(0)
    ).values
    
    return df_chunk

# -------------------------
# 4) Batch Processing Loop
# -------------------------
output_dir = "features_batches"
os.makedirs(output_dir, exist_ok=True)

for batch_idx in range(num_batches):
    start_idx = batch_idx * BATCH_SIZE
    end_idx = min((batch_idx + 1) * BATCH_SIZE, len(customers))
    batch_customers = customers[start_idx:end_idx]
    
    logging.info(f"
=== Batch {batch_idx+1}/{num_batches} ===")
    logging.info(f"Processing customers {start_idx:,} to {end_idx:,}")
    
    # Load only this batch of customers
    df_batch = pd.read_parquet(
        'your_data.parquet',
        filters=[('CUSTOMER_EXTERNAL_CODE', 'in', batch_customers.tolist())],
        dtype=DTYPES
    )
    
    logging.info(f"Loaded {len(df_batch):,} rows for {len(batch_customers):,} customers")
    
    # Create features
    df_features = create_features_chunk(df_batch)
    
    # Save batch
    output_path = os.path.join(output_dir, f"features_batch_{batch_idx:04d}.parquet")
    df_features.to_parquet(output_path, index=False)
    
    logging.info(f"Saved batch to {output_path}")
    
    # Free memory
    del df_batch, df_features
    gc.collect()

logging.info("
âœ“ All batches processed successfully")

# -------------------------
# 5) Optional: Combine Batches for Train/Valid Split
# -------------------------
def combine_batches_for_modeling():
    """
    Load all batches and create train/valid split
    Uses chunked reading to avoid memory issues
    """
    import glob
    
    batch_files = sorted(glob.glob(os.path.join(output_dir, "features_batch_*.parquet")))
    
    N_VALID_WEEKS = 8
    
    for batch_file in batch_files:
        df = pd.read_parquet(batch_file)
        
        max_week = df["WEEK_START_DATE"].max()
        val_start = max_week - pd.to_timedelta(7 * (N_VALID_WEEKS - 1), unit="D")
        df["SPLIT"] = np.where(df["WEEK_START_DATE"] >= val_start, "VALID", "TRAIN")
        
        # Save back with split
        df.to_parquet(batch_file.replace('.parquet', '_with_split.parquet'), index=False)
        del df
        gc.collect()
    
    logging.info("Added train/valid split to all batches")

# Uncomment to add splits
# combine_batches_for_modeling()
