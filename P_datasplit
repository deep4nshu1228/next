# =============================================================================
# Memory-Efficient Feature Engineering for 16M Rows on 8GB RAM
# Uses customer-based chunking to process data in batches
# =============================================================================

import pandas as pd
import numpy as np
import logging
import os
import gc

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")

# -------------------------
# 0) Initial Setup and Memory Optimization
# -------------------------
# Define dtypes upfront to reduce memory on load
DTYPES = {
    'CUSTOMER_EXTERNAL_CODE': 'category',
    'SKU_ERP_CODE': 'category',
    'BASE_PRICE': 'float32',
    'TOTAL_QUANTITY': 'float32',
    'TOTAL_DLP_VALUE': 'float32'
}

DATE_COLS = ['WEEK_START_DATE', 'WEEK_END_DATE']

# -------------------------
# 1) Load Customer List Only (Minimal Memory)
# -------------------------
logging.info("Loading customer list...")
customers = pd.read_parquet(
    'your_data.parquet',
    columns=['CUSTOMER_EXTERNAL_CODE']
)['CUSTOMER_EXTERNAL_CODE'].unique()

logging.info(f"Total customers: {len(customers):,}")

# -------------------------
# 2) Define Batch Size and Chunking Strategy
# -------------------------
BATCH_SIZE = 2000  # Process 2000 customers at a time (adjust based on memory)
num_batches = int(np.ceil(len(customers) / BATCH_SIZE))

logging.info(f"Processing in {num_batches} batches of ~{BATCH_SIZE} customers each")

# -------------------------
# 3) Feature Engineering Function (Memory-Conscious)
# -------------------------
def create_features_chunk(df_chunk):
    """
    Create features for a subset of customers
    Returns only essential features to minimize memory
    """
    GROUP_KEYS = ["CUSTOMER_EXTERNAL_CODE", "SKU_ERP_CODE"]
    TARGET = "TOTAL_QUANTITY"
    
    # Sort for time-series operations
    df_chunk = df_chunk.sort_values(GROUP_KEYS + ["WEEK_START_DATE"])
    
    # Derived fields
    df_chunk["UNIT_DLP_PRICE"] = np.where(
        df_chunk[TARGET] > 0,
        df_chunk["TOTAL_DLP_VALUE"] / df_chunk[TARGET],
        np.nan
    ).astype('float32')
    
    # Temporal features (minimal set)
    iso = df_chunk["WEEK_START_DATE"].dt.isocalendar()
    df_chunk["YEAR"] = iso.year.astype("int16")
    df_chunk["WEEK"] = iso.week.astype("int8")
    df_chunk["MONTH"] = df_chunk["WEEK_START_DATE"].dt.month.astype("int8")
    df_chunk["WEEK_SIN"] = np.sin(2*np.pi*df_chunk["WEEK"]/52.0).astype('float32')
    df_chunk["WEEK_COS"] = np.cos(2*np.pi*df_chunk["WEEK"]/52.0).astype('float32')
    
    # Core lags (limited set for memory)
    def add_lag_features(g):
        for L in [1, 2, 4, 8]:  # Reduced lag set
            g[f"qty_lag{L}"] = g[TARGET].shift(L).astype('float32')
        return g
    
    df_chunk = df_chunk.groupby(GROUP_KEYS, group_keys=False).apply(add_lag_features)
    
    # Rolling features (essential only)
    def add_rolling_features(g):
        s = g[TARGET].shift(1)
        for w in [4, 8]:  # Only 4 and 8 week windows
            g[f"qty_rollmean_{w}"] = s.rolling(w, min_periods=1).mean().astype('float32')
            g[f"qty_rollstd_{w}"] = s.rolling(w, min_periods=2).std().astype('float32')
        return g
    
    df_chunk = df_chunk.groupby(GROUP_KEYS, group_keys=False).apply(add_rolling_features)
    
    # Exponential smoothing (minimal)
    def add_ewm_features(g):
        s = g[TARGET].shift(1)
        g["qty_ewm_mean_h4"] = s.ewm(halflife=4, min_periods=1, adjust=False).mean().astype('float32')
        return g
    
    df_chunk = df_chunk.groupby(GROUP_KEYS, group_keys=False).apply(add_ewm_features)
    
    # Intermittency
    def add_intermittency(g):
        nz = (g[TARGET].shift(1).fillna(0) > 0).astype(int)
        g["nonzero_ratio_8"] = nz.rolling(8, min_periods=1).mean().astype('float32')
        return g
    
    df_chunk = df_chunk.groupby(GROUP_KEYS, group_keys=False).apply(add_intermittency)
    
    # Fill NaNs
    feature_cols = [c for c in df_chunk.columns if c.startswith(('qty_', 'nonzero_'))]
    df_chunk[feature_cols] = df_chunk.groupby(GROUP_KEYS)[feature_cols].apply(
        lambda g: g.fillna(method='ffill').fillna(0)
    ).values
    
    return df_chunk

# -------------------------
# 4) Batch Processing Loop
# -------------------------
output_dir = "features_batches"
os.makedirs(output_dir, exist_ok=True)

for batch_idx in range(num_batches):
    start_idx = batch_idx * BATCH_SIZE
    end_idx = min((batch_idx + 1) * BATCH_SIZE, len(customers))
    batch_customers = customers[start_idx:end_idx]
    
    logging.info(f"
=== Batch {batch_idx+1}/{num_batches} ===")
    logging.info(f"Processing customers {start_idx:,} to {end_idx:,}")
    
    # Load only this batch of customers
    df_batch = pd.read_parquet(
        'your_data.parquet',
        filters=[('CUSTOMER_EXTERNAL_CODE', 'in', batch_customers.tolist())],
        dtype=DTYPES
    )
    
    logging.info(f"Loaded {len(df_batch):,} rows for {len(batch_customers):,} customers")
    
    # Create features
    df_features = create_features_chunk(df_batch)
    
    # Save batch
    output_path = os.path.join(output_dir, f"features_batch_{batch_idx:04d}.parquet")
    df_features.to_parquet(output_path, index=False)
    
    logging.info(f"Saved batch to {output_path}")
    
    # Free memory
    del df_batch, df_features
    gc.collect()

logging.info("
✓ All batches processed successfully")

# -------------------------
# 5) Optional: Combine Batches for Train/Valid Split
# -------------------------
def combine_batches_for_modeling():
    """
    Load all batches and create train/valid split
    Uses chunked reading to avoid memory issues
    """
    import glob
    
    batch_files = sorted(glob.glob(os.path.join(output_dir, "features_batch_*.parquet")))
    
    N_VALID_WEEKS = 8
    
    for batch_file in batch_files:
        df = pd.read_parquet(batch_file)
        
        max_week = df["WEEK_START_DATE"].max()
        val_start = max_week - pd.to_timedelta(7 * (N_VALID_WEEKS - 1), unit="D")
        df["SPLIT"] = np.where(df["WEEK_START_DATE"] >= val_start, "VALID", "TRAIN")
        
        # Save back with split
        df.to_parquet(batch_file.replace('.parquet', '_with_split.parquet'), index=False)
        del df
        gc.collect()
    
    logging.info("Added train/valid split to all batches")

# Uncomment to add splits
# combine_batches_for_modeling()








# =============================================================================
# Join Feature Batches and Create Train/Validation Split
# =============================================================================

import pandas as pd
import numpy as np
import logging
import glob
import os
import gc

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")

# -------------------------
# 1) Join All Batches
# -------------------------
OUTPUT_DIR = "features_batches"
batch_files = sorted(glob.glob(os.path.join(OUTPUT_DIR, "features_batch_*.csv")))

logging.info(f"Found {len(batch_files)} batch files to combine")

# Option A: Load all batches into memory (if they fit in RAM)
def join_all_batches_in_memory():
    """Use if all batches combined fit in your RAM"""
    batch_list = []
    
    for batch_file in batch_files:
        logging.info(f"Loading {batch_file}...")
        df = pd.read_csv(
            batch_file,
            dtype={'CUSTOMER_EXTERNAL_CODE': 'category', 'SKU_ERP_CODE': 'category'},
            parse_dates=['WEEK_START_DATE', 'WEEK_END_DATE']
        )
        batch_list.append(df)
        logging.info(f"  Loaded {len(df):,} rows")
    
    # Combine all batches
    logging.info("
Combining all batches...")
    df_all = pd.concat(batch_list, ignore_index=True)
    del batch_list
    gc.collect()
    
    logging.info(f"✓ Combined dataset: {len(df_all):,} rows, {df_all.shape[1]} columns")
    return df_all

# Option B: Process batches one-by-one and save train/valid separately (memory-efficient)
def join_batches_memory_efficient():
    """Use if combined batches don't fit in RAM - processes incrementally"""
    
    N_VALID_WEEKS = 8
    
    # First pass: find global max week for validation cutoff
    logging.info("Finding global max week...")
    max_week = pd.Timestamp.min
    
    for batch_file in batch_files:
        df_sample = pd.read_csv(batch_file, usecols=['WEEK_START_DATE'], parse_dates=['WEEK_START_DATE'])
        batch_max = df_sample['WEEK_START_DATE'].max()
        if batch_max > max_week:
            max_week = batch_max
        del df_sample
        gc.collect()
    
    val_start = max_week - pd.to_timedelta(7 * (N_VALID_WEEKS - 1), unit="D")
    logging.info(f"Validation window: {val_start.date()} to {max_week.date()}")
    
    # Second pass: split each batch into train/valid and save
    train_files = []
    valid_files = []
    
    for idx, batch_file in enumerate(batch_files):
        logging.info(f"
Processing {batch_file} ({idx+1}/{len(batch_files)})...")
        
        df = pd.read_csv(
            batch_file,
            dtype={'CUSTOMER_EXTERNAL_CODE': 'category', 'SKU_ERP_CODE': 'category'},
            parse_dates=['WEEK_START_DATE', 'WEEK_END_DATE']
        )
        
        # Split into train and validation
        train = df[df['WEEK_START_DATE'] < val_start].copy()
        valid = df[df['WEEK_START_DATE'] >= val_start].copy()
        
        # Save splits
        if len(train) > 0:
            train_file = os.path.join(OUTPUT_DIR, f"train_batch_{idx:04d}.csv")
            train.to_csv(train_file, index=False)
            train_files.append(train_file)
            logging.info(f"  Train: {len(train):,} rows -> {train_file}")
        
        if len(valid) > 0:
            valid_file = os.path.join(OUTPUT_DIR, f"valid_batch_{idx:04d}.csv")
            valid.to_csv(valid_file, index=False)
            valid_files.append(valid_file)
            logging.info(f"  Valid: {len(valid):,} rows -> {valid_file}")
        
        del df, train, valid
        gc.collect()
    
    return train_files, valid_files

# -------------------------
# 2) Create Train/Validation Split
# -------------------------
N_VALID_WEEKS = 8  # Last 8 weeks for validation

# Choose option based on your memory
USE_MEMORY_EFFICIENT = True  # Set False if all batches fit in RAM

if USE_MEMORY_EFFICIENT:
    # Memory-efficient approach - saves train/valid separately per batch
    train_files, valid_files = join_batches_memory_efficient()
    
    logging.info(f"
✓ Created {len(train_files)} train batches and {len(valid_files)} valid batches")
    logging.info("Use these for batch-wise training or combine further as needed")
    
else:
    # Load all in memory approach
    df_all = join_all_batches_in_memory()
    
    # Create train/valid split
    max_week = df_all['WEEK_START_DATE'].max()
    val_start = max_week - pd.to_timedelta(7 * (N_VALID_WEEKS - 1), unit="D")
    
    logging.info(f"
Splitting data...")
    logging.info(f"Validation window: {val_start.date()} to {max_week.date()}")
    
    train = df_all[df_all['WEEK_START_DATE'] < val_start].copy()
    valid = df_all[df_all['WEEK_START_DATE'] >= val_start].copy()
    
    logging.info(f"Train: {len(train):,} rows ({len(train)/len(df_all)*100:.1f}%)")
    logging.info(f"Valid: {len(valid):,} rows ({len(valid)/len(df_all)*100:.1f}%)")
    
    # Save to CSV
    logging.info("
Saving train/valid datasets...")
    train.to_csv(os.path.join(OUTPUT_DIR, "train_full.csv"), index=False)
    valid.to_csv(os.path.join(OUTPUT_DIR, "valid_full.csv"), index=False)
    
    logging.info(f"✓ Saved to {OUTPUT_DIR}/train_full.csv and valid_full.csv")
    
    del df_all, train, valid
    gc.collect()

# -------------------------
# 3) Optional: Combine Train/Valid Batches into Single Files
# -------------------------
def combine_split_batches(file_pattern, output_name):
    """Combine multiple batch files into single CSV"""
    files = sorted(glob.glob(os.path.join(OUTPUT_DIR, file_pattern)))
    
    if len(files) == 0:
        logging.warning(f"No files found matching {file_pattern}")
        return
    
    logging.info(f"
Combining {len(files)} files into {output_name}...")
    
    first = True
    total_rows = 0
    
    for f in files:
        df = pd.read_csv(f)
        total_rows += len(df)
        
        df.to_csv(
            os.path.join(OUTPUT_DIR, output_name),
            mode='w' if first else 'a',
            header=first,
            index=False
        )
        
        first = False
        del df
        gc.collect()
    
    logging.info(f"✓ Combined {total_rows:,} rows into {output_name}")

# Uncomment to combine train/valid batches into single files
# combine_split_batches("train_batch_*.csv", "train_combined.csv")
# combine_split_batches("valid_batch_*.csv", "valid_combined.csv")

logging.info("
" + "="*60)
logging.info("✓ All processing complete!")
logging.info("="*60)
